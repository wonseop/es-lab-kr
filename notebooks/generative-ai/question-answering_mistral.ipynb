{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tZnIXBfrRpex"
      },
      "source": [
        "# Question Answering with Langchain and Mistral\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/elastic/elasticsearch-labs/blob/main/notebooks/generative-ai/question-answering.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "이 대화형 노트북은 Langchain을 사용하여 가상의 직장 문서를 구절로 분할하고 \"multilingual-e5-base\"를 사용하여 이러한 구절을 임베딩으로 변환하고 Elasticsearch에 저장합니다.\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAf4AAACiCAIAAAAvNjhKAAAMQWlDQ1BJQ0MgUHJvZmlsZQAASImVVwdYU8kWnluSkEBCCV1K6E0QqQGkhNACSC+CqIQkQCgxBoKKHV1UcO1iARu6KqLYAbGgiGJhUex9saCirIsFu/ImBXTdV7433zd3/vvPmf+cOXfm3jsAqJ/gisV5qAYA+aJCSVxoIGNMSiqD9BSQAApoQBvYcXkFYlZMTCSAZbD9e3l3HSCy9oqjTOuf/f+1aPIFBTwAkBiIM/gFvHyIDwKAV/HEkkIAiDLeYnKhWIZhBdoSGCDEC2Q4S4GrZDhDgffKbRLi2BC3AqCixuVKsgCgXYI8o4iXBTVofRA7i/hCEQDqDIj98vMn8iFOh9gW2oghlukzM37QyfqbZsaQJpebNYQVc5EXlSBhgTiPO/X/TMf/Lvl50kEf1rCqZUvC4mRzhnm7mTsxQobVIO4VZURFQ6wF8QchX24PMUrJloYlKuxRI14BG+YM6ELszOcGRUBsBHGIKC8qUslnZApDOBDDFYJOERZyEiDWh3iBoCA4XmmzSTIxTukLrc+UsFlK/ixXIvcr83VfmpvIUuq/zhZwlPoYrTg7IRliCsSWRcKkKIhpEDsV5MZHKG1GFWezowZtJNI4WfyWEMcJRKGBCn2sKFMSEqe0L8svGJwvtilbyIlS4v2F2QlhivxgrTyuPH44F+ySQMRKHNQRFIyJHJwLXxAUrJg79kwgSoxX6nwQFwbGKcbiFHFejNIeNxfkhcp4c4jdCorilWPxpEK4IBX6eKa4MCZBESdenMMNj1HEgy8FkYANggADSGHNABNBDhB29Db0wjtFTwjgAgnIAgLgqGQGRyTLe0TwGg+KwZ8QCUDB0LhAea8AFEH+6xCruDqCTHlvkXxELngCcT6IAHnwXiofJRrylgQeQ0b4D+9cWHkw3jxYZf3/nh9kvzMsyEQqGemgR4b6oCUxmBhEDCOGEO1wQ9wP98Ej4TUAVheciXsNzuO7PeEJoZPwkHCN0EW4NUFYIvkpytGgC+qHKHOR8WMucGuo6Y4H4r5QHSrjurghcMTdoB8W7g89u0OWrYxblhXGT9p/m8EPT0NpR3Ymo2Q9cgDZ9ueRNHua+5CKLNc/5kcRa8ZQvtlDPT/7Z/+QfT5sI362xBZgB7A27CR2DjuKNQAG1ow1Yu3YMRkeWl2P5atr0FucPJ5cqCP8h7/BJyvLZIFzrXOP8xdFX6FgiuwdDdgTxVMlwqzsQgYLfhEEDI6I5zSc4eLs4gqA7PuieH29iZV/NxDd9u/c3D8A8G0eGBg48p0LbwZgnyfc/oe/c7ZM+OlQBeDsYZ5UUqTgcNmFAN8S6nCnGQATYAFs4XxcgAfwAQEgGISDaJAAUsB4GH02XOcSMBlMB3NAKSgHS8EqsA5sBFvADrAb7AcN4Cg4Cc6AC+ASuAbuwNXTDV6APvAOfEYQhIRQETpigJgiVogD4oIwET8kGIlE4pAUJB3JQkSIFJmOzEXKkeXIOmQzUoPsQw4jJ5FzSCdyC3mA9CCvkU8ohqqh2qgxao2OQJkoC41AE9BxaBY6CS1G56GL0TVoNboLrUdPohfQa2gX+gLtxwCmiuliZpgjxsTYWDSWimViEmwmVoZVYNVYHdYEn/MVrAvrxT7iRJyOM3BHuILD8ESch0/CZ+KL8HX4Drweb8Wv4A/wPvwbgUowIjgQvAkcwhhCFmEyoZRQQdhGOEQ4DfdSN+EdkUjUJdoQPeFeTCHmEKcRFxHXE/cQTxA7iY+I/SQSyYDkQPIlRZO4pEJSKWktaRepmXSZ1E36oKKqYqriohKikqoiUilRqVDZqXJc5bLKU5XPZA2yFdmbHE3mk6eSl5C3kpvIF8nd5M8UTYoNxZeSQMmhzKGsodRRTlPuUt6oqqqaq3qpxqoKVWerrlHdq3pW9YHqRzUtNXs1tlqamlRtsdp2tRNqt9TeUKlUa2oANZVaSF1MraGeot6nfqDRaU40Do1Pm0WrpNXTLtNeqpPVrdRZ6uPVi9Ur1A+oX1Tv1SBrWGuwNbgaMzUqNQ5r3NDo16RrjtSM1szXXKS5U/Oc5jMtkpa1VrAWX2ue1hatU1qP6Bjdgs6m8+hz6Vvpp+nd2kRtG22Odo52ufZu7Q7tPh0tHTedJJ0pOpU6x3S6dDFda12Obp7uEt39utd1P+kZ67H0BHoL9er0Luu91x+mH6Av0C/T36N/Tf+TAcMg2CDXYJlBg8E9Q9zQ3jDWcLLhBsPThr3DtIf5DOMNKxu2f9htI9TI3ijOaJrRFqN2o35jE+NQY7HxWuNTxr0muiYBJjkmK02Om/SY0k39TIWmK02bTZ8zdBgsRh5jDaOV0WdmZBZmJjXbbNZh9tncxjzRvMR8j/k9C4oF0yLTYqVFi0WfpanlaMvplrWWt63IVkyrbKvVVm1W761trJOt51s3WD+z0bfh2BTb1NrctaXa+ttOsq22vWpHtGPa5dqtt7tkj9q722fbV9pfdEAdPByEDusdOocThnsNFw2vHn7DUc2R5VjkWOv4wEnXKdKpxKnB6eUIyxGpI5aNaBvxzdndOc95q/OdkVojw0eWjGwa+drF3oXnUuly1ZXqGuI6y7XR9ZWbg5vAbYPbTXe6+2j3+e4t7l89PD0kHnUePZ6WnumeVZ43mNrMGOYi5lkvgleg1yyvo14fvT28C733e//l4+iT67PT59kom1GCUVtHPfI19+X6bvbt8mP4pftt8uvyN/Pn+lf7PwywCOAHbAt4yrJj5bB2sV4GOgdKAg8Fvmd7s2ewTwRhQaFBZUEdwVrBicHrgu+HmIdkhdSG9IW6h04LPRFGCIsIWxZ2g2PM4XFqOH3hnuEzwlsj1CLiI9ZFPIy0j5RENo1GR4ePXjH6bpRVlCiqIRpEc6JXRN+LsYmZFHMklhgbE1sZ+yRuZNz0uLZ4evyE+J3x7xICE5Yk3Em0TZQmtiSpJ6Ul1SS9Tw5KXp7cNWbEmBljLqQYpghTGlNJqUmp21L7xwaPXTW2O809rTTt+jibcVPGnRtvOD5v/LEJ6hO4Ew6kE9KT03emf+FGc6u5/RmcjKqMPh6bt5r3gh/AX8nvEfgKlgueZvpmLs98luWbtSKrJ9s/uyK7V8gWrhO+ygnL2ZjzPjc6d3vuQF5y3p58lfz0/MMiLVGuqHWiycQpEzvFDuJScdck70mrJvVJIiTbCpCCcQWNhdrwR75daiv9RfqgyK+osujD5KTJB6ZoThFNaZ9qP3Xh1KfFIcW/TcOn8aa1TDebPmf6gxmsGZtnIjMzZrbMspg1b1b37NDZO+ZQ5uTO+b3EuWR5ydu5yXOb5hnPmz3v0S+hv9SW0kolpTfm+8zfuABfIFzQsdB14dqF38r4ZefLncsryr8s4i06/+vIX9f8OrA4c3HHEo8lG5YSl4qWXl/mv2zHcs3lxcsfrRi9on4lY2XZyrerJqw6V+FWsXE1ZbV0ddeayDWNay3XLl37ZV32umuVgZV7qoyqFla9X89ff3lDwIa6jcYbyzd+2iTcdHNz6Ob6auvqii3ELUVbnmxN2tr2G/O3mm2G28q3fd0u2t61I25Ha41nTc1Oo51LatFaaW3PrrRdl3YH7W6sc6zbvEd3T/lesFe69/m+9H3X90fsbznAPFB30Opg1SH6obJ6pH5qfV9DdkNXY0pj5+Hwwy1NPk2Hjjgd2X7U7GjlMZ1jS45Tjs87PtBc3Nx/Qnyi92TWyUctE1runBpz6mprbGvH6YjTZ8+EnDnVxmprPut79ug573OHzzPPN1zwuFDf7t5+6Hf33w91eHTUX/S82HjJ61JT56jO45f9L5+8EnTlzFXO1QvXoq51Xk+8fvNG2o2um/ybz27l3Xp1u+j25zuz7xLult3TuFdx3+h+9R92f+zp8ug69iDoQfvD+Id3HvEevXhc8PhL97wn1CcVT02f1jxzeXa0J6Tn0vOxz7tfiF987i39U/PPqpe2Lw/+FfBXe9+Yvu5XklcDrxe9MXiz/a3b25b+mP777/LffX5f9sHgw46PzI9tn5I/Pf08+Qvpy5qvdl+bvkV8uzuQPzAg5kq48l8BDFY0MxOA19sBoKYAQIfnM8pYxflPXhDFmVWOwH/CijOivHgAUAf/32N74d/NDQD2boXHL6ivngZADBWABC+AuroO1cGzmvxcKStEeA7YFP01Iz8D/JuiOHP+EPfPLZCpuoGf238Bk9B8XQo0or4AAAA4ZVhJZk1NACoAAAAIAAGHaQAEAAAAAQAAABoAAAAAAAKgAgAEAAAAAQAAAf6gAwAEAAAAAQAAAKIAAAAAGoXaLQAAJsNJREFUeAHtnXfYFcX1x1FBKbHgg6KigMZEsKMoMSoqalBUxBIhWGOMsfcSEwvGhr0QBPNYiCZKRIEQwPAEFRGNogRRihgLigJBjRpRVCy/T5gn6/5umbt3Z+fu7rvf/eO+80495zuzZ2fOnDmzyjfffNNMjxAQAkJACBQJgVWLxKx4FQJCQAgIgf8iINGvcSAEhIAQKBwCEv2F63IxLASEgBCQ6NcYEAJCQAgUDgGJ/sJ1uRgWAkJACEj0awwIASEgBAqHgER/4bpcDAsBISAEJPo1BoSAEBAChUNAor9wXS6GhYAQEAIS/RoDQkAICIHCISDRX7guF8NCQAgIAYl+jQEhIASEQOEQkOgvXJeLYSEgBISARL/GgBAQAkKgcAhI9Beuy8WwEBACQkCiX2NACAgBIVA4BCT6C9flYlgICAEhINGvMSAEhIAQKBwCEv2F63IxLASEgBCQ6NcYEAJCQAgUDgGJ/sJ1uRgWAkJACEj0awwIASEgBAqHgER/4bpcDAsBISAEJPo1BoSAEBAChUNAor9wXS6GhYAQEAIS/RoDQkAICIHCISDRX7guF8NCQAgIAYl+jQEhIASEQOEQkOgvXJeLYSEgBISARL/GgBAQAkKgcAhI9Beuy8WwEBACQqB5PAhWWWWVeAXzW+qbb77JL/HZpDzFUaTezOaQEFUNQyCm6Ie+Qr08KQqphg2FVBpKZRSpN1PpazWaKQSk8MlUd9RHzPvvv//BBx/UV0a5hUCzZp9++um8efPee+89gVFYBCT6c9n1kydP3nfffdu1a7fuuuvutttu48ePr8bG1VdfffTRR/OqM9V99dVXP//887vvvpt/w/kHDRp03HHHhWOCcMX8QaoCuUOAuUL//v3btGmz5ZZbrrfeevvtt9+iRYvicaGxEQ+3jJSS6M9IR9RBBoIbuf+jH/1o2bJlb731FqL/oIMOqjb9R6Py9ddft2zZ8oknnthoo40++eSTn/3sZx9++GG4vWOPPfaXv/xlOCYIV8wfpCqQLwQYDH379n3nnXdee+01wi+//DJD6LDDDovHhcZGPNwyUkqiPyMdUQcZZprWvXt35m6bbLLJZZdddumlly5fvvzRRx894YQTzjzzzPbt2++1117z588PKl2xYsWvf/3rf//73+Y933///d99990g9bHHHnv44Yc/++yzHj163HXXXVtttdV3v/vdO++8kwzh/KgIqHbNNdck21NPPRUUVyAvCEydOnXatGl08WabbQbNW2yxxR133LHTTjshxO+55x5GEdOCX/ziFySNGDFiu+22Yxgwur788kti/vGPfxx++OEMrQMPPPDJJ58kJjw2lixZwmKCVCYlM2fOJFVP1hHg4x/jgasYpfJbJFP8Isd5db/zne+cccYZY8eORaAbYB944AHo/PnPf86byTqgX79+xF955ZUDBw7k3SbplVdemTRpEoHRo0ezWg+6g3cepZDJ8/3vf588iACy8TkJ8v/nP//ZeOON+WZQ+cUXX0zr//rXv4Ia4gVoIl5Bx1JptetItnvxYcOGrb/++hXrYZwAS+/evZkHTJw4kfBtt932t7/9jU6//PLLKcLAOPHEE5977rlzzz0XZRFLyWBsMGnYeeed99lnH/Kb5SNr0IqtKDI7CMR894r28mSNXyZZ11133bbbbgthPGefffZXX31lRD/vIcMLRRDxSPMS0c/OMPEs+cNDMCz6efNJQtCTjWl+kP+RRx4hhnhTENF///33hyuJEabCGKXci6TVrjvljjWw8YMEr1gJ44SvAqOIVOYNfPtNNpYIpgij66OPPmIA3HfffQDItCAYG88//zwxb7zxBkX4JFDPQw89ZIrrN7MIxDfupLP1pIIAwh1Vz/krH74BaGYuueSSPffcE2L4GKyxxhoEOnTowO+LL77Ib10PGiTyo9Xhl5VBUPbNN99krmfiiUQbwKw/SFUgFwhsuummrPxYJmIdYAhGlP/ud787/vjj+Zdp+6qrrkoAVeFf/vIXhL7Jw2eeAKW+973vLV261CiLTJL5ff311wlQeRAZVicGkQpkCoH/9rSefCHwxz/+ESWsoXmDDTZA/cKq3Lx+vJ8mnjecAPOvellbbbXVKhbp2rUrKwlmhaQykZk1axZr/Io5FZlZBOhEaGPDP6AQhSFaoHXWWSeIIcCH4cILL2TJyPP2229PmTKFbeFTTz31+uuvZ7I/ZsyYcGbCa6+9Nr/MQkwRlEKxt45Lata//hDwMutvkkdm7EwhDf11UknNu+yyC5OvW2+99eSTT27evDkqV95P5vtEEkAzs8cee9x+++3I/Y4dO5aUNdM6VLFY+5QkVfw3yE+jZGCFgSoAlS5h36LfDnhFaqNH2itvZG9Gp9k9J2u1H//4x+eddx6LQgYMMhqBjqrH9HJQP9P/Bx98kE0jFnmnnXYa5mFoFEklHtx++9vfEkaxE4yNHXbYgRgUQWw+MTbYB549ezaWo0GFuQssXLhwwYIFrHRRjfJJ483iDAR2cai8zOcNK7svvvgiGCdAsfrqq7M8atWq1VprrcW3kM8nCLDvzYvGzKxTp06slko+senC4kX0w1IASrrsNaZ1uxxJnAYUL4hgjHnOOussKmdgocPt1avXyJEjGXxY6DNSiUQ5y4cB2oIXmzCDb9ddd916660Z02HpH+QJU1uSf/DgwSeddBKCA4tAtAFUHs7sI5zKKGpwb/rAzVLnkCFDkOnYaJk8CGs2bwmHxwkbuU8//fTmm29OPB/4UaNGoQY84IADjBaRUcesgkpYfQZjibE3YMAAdJAUYTRiJGbqz8UvAp3tCgyTUJDOnTsXm1fEN5IaeQ3L/GIExRmatm3bItN5xVq3bs3nsEWLFmaJzChlNcyXgCURnwR0aHwkWH+j9UIpyvqb/TMW5TyUxaqK93ebbbbZfvvtd9xxRz4YaUG0Sry3i4FiKWhPTYtVf+2mwi+DjMHEvIyhaVjj9bv55pt5aZmwoHitKM1NTmb9jOPogAT5GdMsLHgrGP3Ri1fLacfNnlqtTvf4tNp1pzx6DezQMpllkFTrR95u5rxs9rDHCyCmZubCTGbZZ2K3icfMYYOxwXSYnV5kZV1DKzrNyeZE3GMMjS4La9c5c+bwhUMQsyrio4VaLNjTSrZRzLLZR2FJxDeGL82MGTNYTP/whz/s2bMnUzc+DMk2Z6/NJsEtJe2vhz3VUm1OkzLCrxH9zz77bF5gtONmT/XHY1rt+uNINQcI8D1je4PT78zEsWTlnApiN1gDBdkaE8BK++9//zvW0nyBOJ/PIU2WVpy569y5cwMIkOhPAOSMCAvWqtOnT6/mkiEBPpOuwo6bPTVpWr6tL612v6VAoaQRQBWDLTJzIybahxxyCOK1T58+aGySbid+fSyw2KXDsIpddNYfP/nJT4466iiv6iCJ/vi9FZSUsAigqCtgx82eWldDdWVOq926iFTmiAjgtwrrVfbGsH3gbCO73BELppiNUxHsmbMgYBuG3TX27XwQI+NOH6iqTiEgBFJGAFvk008/nW1V9i3wQsFsOhdyH9RwmPHnP/8ZXylsSGCkxwFpNlQSR1OiP3FIU66QgW6crqRMh5rPMAJYpPDUS2D5uMLEszyy3mp95L/hhhvYoMbUEqHJcYTGaM+TZYQDdJhjYVXBBwCjoOCEXVKtSPQnhWRW6sFuIXyusi6y0DZi5FBXEWVOFwG6Gw1VyVPTDzPW+sZpT3TisUspV46jSzniiCMwNoMAVCvRa/OXk41cNkvZxX3hhRdwdoKZpr+2GlAzFticucPpFlAfc8wxQJ1UoxL9SSGpeoRACggYG+uVVuPf/nCSqJGkBC7BG9loxbbYxcVK8gc/+AEe6Lp06VIxTx4jsQHFcg9rWk7VYWKbCAspiP6SGUoT+JeesHORSFdVq4RNIRSavO0cz8HammycLvnVr37FMRxMBYxrraFDhwazPE5jXnvtteQsd9EcNIH9PlYQnOEihszUT23UYARNkC3FgB1wl1SYshdPketqTbPOCz8cNcIbMyezmCfScQTMIKEfJ0yYYCrBmH333XfHgP3II480qmQs/csdL1OQUcRQ+cMf/mAKouTBlydV4Z/ZWBIHLsExMKOG3/zmN2bsPfPMMxRBs3TFFVeQH9Oaq666ygwqNO/GATgUcgaqGl91xfPpO/TQQy+66CJcWtVVMC+Z8aWKYRI7ARyhSIBmXuYYDw1bSrmkWqrNaZIdDUemXnrpJepnTKCrQbmJYYDxssBrhqEYMyDcutEELlkCX4yETznlFDN6KBJ20cxcifP9HNbllCb+mfk88PZydBNjA84Gc+SEV92R4HBxOzIuqeFWkg3bqUq2rSi1me7GcjF4Hn/8cQrijRVS+UVRQIBOpAdx08ZxPFKxGyESGc2AIYl/Eegca2JSSYWB42XOBpKNKcXvf/97shGmrBkJN910kxHijDQzlji2Gow9AgwhLhEi/7hx4xg5eIagOWrAzMY4dyOGbIy3G2+8kWzuDzeOXXPNNe71ZLwGLuTgbiV3Ir2fxaez9fhDgMP0vGBYMtAEU3vjq5kwrzSnLpn9oYq1tD58+HAmXywn2UQy7zlyH0f/zOOY7uEElApxC4GGkbkGe025dsxiwSHvSexqBiwwHowbV4Qv03PicfDHxx5HC+wccj0nnUskWhEzO2bSzWwAvyDM2TmOy47o3nvvTTZ2EUhCNOMtivyEzTBjfoDRofHqU/H84L333ousZ/vXWNSQHzJM2OQ3CmucHDC9YIKC0X1AfOzAn/70J4ZutcvmYlebwYK33HILSyh6h25yIS8FhY8LuSpbggCLXJyBmEimbMb7LhM05D6RLOd5H0qKoA4KYliYm2z8GhfNVMjBQj4DxnKDFTRTDAYZp/P5PBgfjUFxBTKCADru4DGSGsKMEx4C6IiR4ATMESFj28NoMcSzsc8gMZu0aI1QduEChO89c3M2SznsarLhx8YE0NUYX378ixMCExn8MvaMQwL2V83YY4sSUWUymIIYLPKt4kPC+OSbZPE4ElRbM8AcyCxlauZsAhmAjnWYIyMS/Y4AplwclyOYfxkicMTIa0m44iHAwJiPmV1AdLmLZl7dxYsXU4NZnlM563p8UTEN5DAkL1hQVoGMIxD2r4dAL6EWV5Qm5p///CcLBeN4p8TxMh+PYHQFBjx8KvD9Z8oa3+DhmsvHHl8CrG5MHuongAehgw8+GHUinxYsFwcNGhSuIV4YnWTwMYtXQ45KofFHredIsES/I4ApF2cQMEnn9AevE7MeXqSKBDHP4l5WhD5nW/BhUjGPieQCAB62gtHA4mSKzMYzBB7Y2Rgw28iW4kpKBQF6KvxEUaGwbYs0Ryhz0hVryMDxMh8MxApzfCb+xDO6+JfvBLf1GtYYCah0KMtGMSePavKL9okpKmOPdYmphEa7devG9wNlI9tR1QZtzZqDDCi+obDBdk1B640PsGGDgs6xXYl+RwBTLs4bixaeWRtOZXGaiD63IkG4UEfPw4hhtkXminnCE0OyMYcyCmLecPw1ourhBcMapGJZRaaFgOk1ZuLhh4l8CT3hzjVhdm5Q/aPZ//jjj9HpMX5Y1eF1mQ0eRotxvMzowscZ0pk9HqPGoVrGGDMAyuL6my9EoK4JNxFuHcMw6mHsMU2BSJYF/IuWn+uGENY0ijVaOH+MsGm6ONfGwan7UikFHz70E0IkRgfntEgD+GXKj+rGONGthhImHGhyOCFS7RWtVpA9ADYAUAiwGqiWJ168HRmX1Hj0RCllpypKDdnJgwRHA4PT72BIVHS8zMYPlvvh3mcfiHu7GEso62uyw5KUkYnuiI8KK0g+GFz5QCnm/kxH+ITUrCFKBlhgHVOQqQl6V5y7OUrRmFLY/gK4pEbp5nzlsaORL16SpdaOjEtqsnSGa7NTFc6psEHALCa44IXDIlinYC7s4yIX+oXlLA6QiwA7BzK4ZsBR9EvhU4ShIh6FQGoIYF6MKwJWnCwduEfIh9w3vKGSMkcNUmO1IQ3DY6B8c2lQs34X9CKV1TyxGkx2ZBxTqzXqO95xLuabvKZaP6MFHRT7xhxWcN8CzSxK7MlzNJqNd3ZKHEda88wyKcKEgAsCji9GvKYRQPEKqpQ7Auwkc4aARQYrjCbpywGhj4kUPMKpO1xS+LhjqBqEgBDIBAK4G+KsANsJWBNxu3omaEqCCHiBI/iCO3hMospmEv2JwFjoSi677LJC8y/m60HA92jBYAkXdRikcsr9ggsuSMo3XD0sJpkX+uECXuAIvuAuqdol+pNCsqD1cPSfc7/48Coo/2K7HgQaNlqwH+WkMUfbMErmsILxT1UPpennhWYoh364gBdjEZsgWRL9CYJZxKo4E4R/GM58Gi8xRYRAPEdGoJGjhRPs3HI1f/58NmA4RMbsZNSoUZEpTTMjdEItNEM59MMFvCROkER/4pAWrkLO+uLkh0OhnPk0TqQLB4EYjoxAg0cLR8m4qwunERxRxn81reO9HP8Txn1pZKq9Z4QeqII2KIROqIVmKA988CVOgUR/4pAWsUKu7sNJCybbKCVvv/32IkIgniMj0PjRwiW9SFWuB5g5cybLU1xUcciAjVOu7a3odzoyK64ZaR0aoAR6oAraoBA6oRaaXWu3lpddvxWeJBJZtaViaJgE7bXrwJKaTIEfQdz+cH6HgYsBcs3CdmT8pdYkLHYGO82xq20yBV1Gix2EepHHZxwu76dMmcKxWLxUcU0NljO4GGL60rVr1ygOKuz0VExlZTxv3jyamzVrFs7suCCB5jiEjIc7/KLX5RG9Xn7L6YkplewNu6SWk5j3GDsaeeeu5GWGHdw7c5UHN4HwUtm5syPjL9VOlUuqnWaXmptGWZfRYkfABXk+A9xjynQb76dz587FkpLLBrCdx5wGl4i4KsKTOe7t2HFFOnOStnXr1kzSWbsEPs/xiI7GBp9I3ELDLQVUyJ2XOBPF++miRYtQ3eCwCEdYmOtwYzCO85jd47u0e/fudYn7MAIu/Jp6JPrDeHoJu3eSF7ISqrT8ZaZizlVyiQfuuniXLE7lQMZOhWW1ZEe1Zqq9XX+pFo78NZqdml1Gi50Le4/by5ancvU5BjbIa6Q2Hu6Q4MhxfBAh0/Fwx4OIx/dc0Ju0jkdSPglckcSDQGfY87Xgm4HbO74ffEXY4jY3I5U3FyPGnV+d5o0Bu4rUQIBT5viVxURhww035M5YzBUqFgjenIqpDO6K8YlE2ptOpInySrxyVN5cXmIijpZGsoOM5sFLWs1GGUg57VZt89bsXGWIiQDXwvTt25crHotwY2pMjFTsfwjkdLTkVO6Dumb9/xt6+usBAS7LxuaHuzhmz549fvx4Dy2oyqaDgEZLQ/uSBUuMBxItpVxSLdXmNMmORpgpl5t3MBKgoVQemg5zUR7GjoJ9M3a3ypMsMfCSSqqlUcckO0f1Vq7REkYsWWzDNWcz7M6vtnnB0O/DkpDRE6WNyy+/nGy+nZxEocRHHq72Za/sr3/9K4Z0Ueq34+YvNQpt8fLYaa63To2WMGLJYhuuOZthd36l689mzzZBqvBDguk0Rp8333xzE2RPLCWKgEZLonBWqEyivwIoivKEAMcUzzzzzHPOOeeYY47x1ISqbTIIaLR47UqJfq/wqvJSBJjyc6k0G3o9evQoTdP/QuD/I6DR8v/xSPI/if4k0VRdURAYOHAgl04899xz99xzT5T8ylNkBDRaPPW+RL8nYFVtVQQ45IXGf6eddvrpT39aNdPKBPayqj32gi6p1Vp0j4cqeyUuZDfVstFHS1NFwBNfEv2egFW1lRE4++yzjzzySC5Qrekx0W5UV7n2hGLtTXtKTYj2JlVN9NHSpNhuCDM60tUQmNXISgT23Xdf3CXedNNNvNKCRAjYEdBosePjmCrR7wigikdFwNj1o+WPaNcftV7la4oIaLT47lUpfHwjXF/9HNG0q4OrpeITsVqS73jjjtHC5xNPPIEvQzzccpmX5L4FqHqTNFrqRUz5v0UgnuKS8paCLqmjR48OiMM1du/evfHrZGmrYUk4477rrrvw11pvi3Y06q0ta/nx4mB35HD11VeDwAEHHJAs5XZU/aUmy0W4NjvN4Zz5DfsbLUVAL9zv7vxmbtYPe3CFy18eDMDxgo3vX+5PIDLdB6HP5VP47E6XjHy13r9/f3y3XXjhhfLdlq+OS4VajZZGwp450W+Y54oDHlz+jhs3DkNANgaJ524zdAvcncZpoKeeesrkfOihh7bYYov27dufccYZTMyffvppSpkk8hx++OGE77jjjnPPPZcTpPjgJmCKUGrChAkm54gRI7ibDb/h+M/58ssvqYcmmONzfRqR5q7Bww47jMz777//u+++O2bMGEMJdXLzjqlEvyUI8M3mpmmWcYMHDy5J0r9CoAQBjZYSQLz/G15ERA9DliWzS+rDDz9cUhyFJgpixPHGG2+M5H3yyScvvvhidEEsC1566SUy33bbbY888gj7Qvfdd9/EiRPJZmhjpskta4QvvfRSsvHLVfcE+Kg88MADxx9/vEmliKmEg+OUxSsWE3xiqHDSpEnM9AkvX76cMAEEGRf3EHjwwQfJz0VrN954owUKkshsz5Dr1IpL+FdffZXLifjQck2dJ+7sqPpL9cQO1dpp9tduI2v2N1qKgF64p9z5jSmV7A27pJaL/iFDhuDvF+FOtUyxDf+Ifs56cAcIlxqbGAQx8/Rqon/XXXc12ZD7fEsIcxsnFXLL2kEHHYR8N6nUgMQ3ov+xxx4j0kzqWXC8//775DfXbBKAKi5Z5go3bvI0Zav9krlaUhOIL3+ZwZAdXS5o9MqdHVV/qf6YstPsr91G1uxvtBQBvXBPufObUYUPjAUP1xnj8REhywcAbY+JRz/DrJ8kbgIxMfvssw8T+aAUAVQ3wb+bb765CXNnJlN1wuwi8Mt9yvPnz0daGTMYvgFco2xyMmklYFr8/PPPTSS/HTt2vOGGG04//XSSTjnllFVXzQGGAfG+A6eeeioYHnfccYFGzl+LFsulVBq10BMlCZrt2fwxlVbNjRwtafGY2XazLrbQtLDZi+jv2rXrW2+9haQGSr5+s2bNQguELv7tt9824OITBhU8YSbyJob8JsBv8+bfnmDgBQviCay77rrsQzLT56E27hUxqUxdw9mCMNP/gw8+GAUUywKuaWYNESQVPMD+x/Dhw4cOHWp2R7yiEZ4BlYdTbLqcmERivHKUSuWNHC2pMJjxRjMq+pmJ8zBz5EQfup1jjz12l112AUpkCnN5rvsgjOjv06fP5MmTyYY4PumkkxDEHTp0WLp06dSpU7kVBDEUBX2WC3wzFi9evGzZstNOO40ZfcVSZnaP8pr1R7du3dD8MHZ79epFoxXzFypyxYoVm266KbcwvvDCC6yECsW7mK0XAY2WehHzkT9zot9MybusfPbbbz92YlHfo6Vp0aIFhiLI97Zt2yLxb731VibyWAVgw4O6v127dgj9AQMGoBRClO+xxx5IIrT2FSELz/oJY/PD7i4aIcyE0PZce+215aXIBg1sGGy99dZsYLLbjOUP+UeOHInxYnn+QsVwUIuVE+ovbJ+22WabQvEuZutFIJXRwtvKyj4gFcUAw/W6664LYiwBpnqjRo2yZLAkuZS1VJtMUrzVKG1bCrqkWqoliUk3Vj1oZsLZmOCXWJIwhUc7H85jD3/99ddvvPEGG7YE7DkZuCbDggULuEjIntmk2tGIUkOW83Tu3BkG+/Xrlyki7Zj7S/UHgp1mf+0mW7O/0WLHB7M9pnfB241JCPmxQ4vCHfblxhQwSuaSPC5lS6oq+dfOb0nmiv/aJHjFAibS3rBLqqXRnCbZ0cgpU2GysZoN/5uFsB1zf6n+eLfT7K/dxGv2NFrs+JgzodOnTzfsnHzyyZwWIoz7WHQGWP0dffTRzCCJQRnF4R5i0B9gxYchCXKfyjnWQyrnVIhHBc1cZ+HChcTcfffdl1xyCQYmJ554IvNCTheZsgj9uspSFddXYIFCc0AEGcRYHju/loJBkkR/AIWvgHsn+aKs6dZrx9xfqj9E7TT7azcvNdfEB+UwtuCwg1RFdg8bNmzJkiWUwlQPH1M4GjFm4mwQksqRIA4JkTpnzhw8kSDNZ8yYQZiYs846CxeE5Ef9yzLiyiuvJBJ/Mxh9XH/99QSwGjeRfAmily0/XWRHnkbtGWqmxixvb9gltSbFuctgRyN37OSCYDvm/lL9gWOn2V+7eam5Jj633HKL0fkg6MmMaTi3PwZaIJS9RKIoxngE7ZDhms8DmQOlzUUXXWTWCqRy0If8qPKR8nwYsDwkEgMHPjBopPkkcPIUvXT0suWni+zI1+TXXpzUzG3zwpIeISAEhECyCGAPgun2zJkzx44dy5wdef3aa68Rg+UeRhyYldAcOh9UQ9iRm6YxKunZs2dABvmNnSExnTp14he7Bn6xKzHmf3wbWrVqhaUDNiOYurVu3Tp6WQwaK54uCmpIPCDRnzikqlAICIHMIYAFIKbY+O/inNBRRx0FfdjsMcf/73GeTz7BLeO0adOwCeSIaHCoE68t5sy/YQaZHhwVYouYSOz9wnxSCe5eWAqgFEKPZFYGEctWO10Urj/ZsER/sniqtqwgwFSu2pMVEkVHYxFgL/eaa67h3A+zflrefffd2fhFuDNOMBY/9NBDmbzze++992K/h2afrV3umSCSEz8cJ8KmHMnOF4KyeJFBRbPGGmuEOWC/F9PwDTfckCtIiefUZ/SyEU8XhZtzDddUCVXMQKsV402kS6ql2pwm2dHIKVO5JtveIy6p/mCxU+Wv3bzUHAUf9l3JxpQ/YOqcc84hhgf9D1eHEs8eAPp6YtjsxU0kMRzeJJUFARp8Tg6ZJGKwDiL1qquuCipkp9ekUta4BYteFvN0FiUU52EtwtIhILJigGwV46NHrkLWlc3V98N30lLQJbU+OvKQ245GHjhoajTae8Ql1R9Sdqr8tZuXmmPjg74ecY92vmXLlgGz7AEg3I2bLyKZ8uNRhlNghFEH4beRM2Jh3zBBQbw9smIgtU2bNiYyelkkKhKfA0l8ZmAnqLNiIDa/QW02CR5kKg/YG3ZJLW8r7zF2NPLOXR7pt/eIS6o/NOxU+Ws3LzUXDR93fqXrz8vYFp1CQAgIgcQQkOhPDEpVJASEgBDICwIS/XnpKdEpBISAEEgMAYn+xKBURUJACAiBvCAg0Z+XnhKdQkAICIHEEJDoTwxKVSQEhIAQyAsC315bWC/FWBfVW0T5hUBGENDozUhHiIy0EIgp+i3nueBE71Va3al2oyCg0RsFJeVp2ghI4dO0+1fcCQEhIAQqICDRXwEURQkBISAEmjYCEv1Nu3/FnRAQAkKgAgIS/RVAUZQQEAL5QmDQoEH5ItiRWnd+Y7pvs9Ntdy1kT7XXnMfUovGbxz4K02zvL3tquJ5kw2m1mywX/mpbb731Zs+e3b59e39NZKdmXI1yS4y5Iyw2VZr1x4ZOBYWAEMgKAlydOHny5KxQ45kOOA2uiozdlER/bOhUUAgIgawgwMVYw4cPzwo1numAU3MRmEs7Ev0u6KmsEBACmUCgf//+3I01ePDgTFDjkwh4hFP4dWwk5pGumq2imqyZRxmEQDYR0OjNZr/YqRo6dOhee+3Vrl27E044wZ4zv6l33nnnsGHDHn/8cXcWvIh+nZZ07xjVkBYCGr1pIe/Y7mabbTZ69Ogjjjhi8eLF3JDuWFsGi19xxRUjRoyARzh1J08KH3cMVYMQEAKZQGDHHXfkbvRnnnmmT58+L7/8ciZoSoIIeIEj+II7eEyiymYS/YnAqEqEgBDIBAKdOnWaMGFCr169tt9++wsuuIB70jNBVlwioB8u4AWO4Avu4tZUWk6ivxQR/S8EhEDeETjvvPNeeeWV5cuXt23b9vzzz1+wYEHuOIJmKId+uIAXOEqWBYn+ZPFUbUJACGQCgY4dOw4ZMmT+/Pls2u+www6HHHLIqFGjMkFZLSKgE2qhGcqhHy7gpVahutO9nOa1UwE/9p00e/HcpRaN39x1UF0Ep9WbabVbFziZzczE+f777x85cuSMGTOQqn379kV13qJFi+wQvGLFiokTJ44bN27MmDFo8wcMGDBw4MBWrVr5ozAFKVy0QVw0fv0N1izUnFZvptVuFjBPkIY333xz7Nix48ePZ7+0d+/eGIP27NmzR48eCTZRV1XPPvvs1KlTMdacNGkS2vwDDzywX79+CSr0LcRI9FvASSZJL20yOGajlrR6M612s4F68lR89NFHjz766JQpU6ZNmzZnzpydd96ZufZ222231VZbde3adc0110y+yWbNPv7443nz5tHcrFmzWH9Mnz6d5nbbbbc999xz7733XnvttX00Wq1Oif5qyCQWr5c2MSgzUFFavZlWuxmA3DsJfAaef/75mTNnvvjii3PnzsWScq211sJ2ntl3hw4dNtpoo/XXX5+TYuy4Ip05Sdu6deuWLVuiL1pttdUMcV999RUam88+++zTTz9dtmwZFX7wwQfvvffe0qVLFy1a9M4777DaeP311zHX6dKly5Zbbrntttt269ate/fuDRb3YSjTEf1hCooQLtTeRtPuUERwWgxqFDUM+YULF2Jgg7xGai9ZsgQJjhz/8MMPkemfrHwQ8V988UXQI4yK1VdfnU9Cm5UPAn2dddbha8E3Y4MNNuD7wVekc+fOm2yyScNYqNlQCqK/Jk3KIASEgBDIBQJI/xRnAy4QSfS7oKeyQkAICIFcIiC7/lx2m4gWAkJACLggINHvgp7KCgEhIARyiYBEfy67TUQLASEgBFwQkOh3QU9lhYAQEAK5RECiP5fdJqKFgBAQAi4ISPS7oKeyQkAICIFcIiDRn8tuE9FCQAgIARcEJPpd0FNZISAEhEAuEZDoz2W3iWghIASEgAsCEv0u6KmsEBACQiCXCEj057LbRLQQEAJCwAUBiX4X9FRWCAgBIZBLBCT6c9ltIloICAEh4IKARL8LeiorBISAEMglAhL9uew2ES0EhIAQcEFAot8FPZUVAkJACOQSAYn+XHabiBYCQkAIuCAg0e+CnsoKASEgBHKJgER/LrtNRAsBISAEXBCQ6HdBT2WFgBAQArlEQKI/l90mooWAEBACLghI9Lugp7JCQAgIgVwiINGfy24T0UJACAgBFwQk+l3QU1khIASEQC4RkOjPZbeJaCEgBISACwIS/S7oqawQEAJCIJcISPTnsttEtBAQAkLABYH/AzFaA2FwdiyIAAAAAElFTkSuQmCC)\n",
        "\n",
        "그런 다음 질문을 하면 벡터 스토어에서 관련 구절을 검색하고 langchain과 Mistral LLM을 사용하여 질문에 대한 요약을 제공합니다."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GyAst2W-VpHb"
      },
      "source": [
        "## Install packages and import modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "33A-cP-XvFCr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (0.0.340)\n",
            "Requirement already satisfied: elasticsearch in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (8.11.0)\n",
            "Requirement already satisfied: tiktoken in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (0.5.1)\n",
            "Requirement already satisfied: sentence_transformers in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (2.2.2)\n",
            "Requirement already satisfied: llama-cpp-python in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (0.2.19)\n",
            "Requirement already satisfied: wget in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (3.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from langchain) (3.9.0)\n",
            "Requirement already satisfied: anyio<4.0 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from langchain) (0.6.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from langchain) (0.0.66)\n",
            "Requirement already satisfied: numpy<2,>=1 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from langchain) (1.26.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from langchain) (2.5.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: elastic-transport<9,>=8 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from elasticsearch) (8.10.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from tiktoken) (2023.10.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from sentence_transformers) (2.1.1)\n",
            "Requirement already satisfied: torchvision in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from sentence_transformers) (0.16.1)\n",
            "Requirement already satisfied: scikit-learn in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from sentence_transformers) (0.19.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from llama-cpp-python) (4.8.0)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from anyio<4.0->langchain) (3.5)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.2 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from elastic-transport<9,>=8->elasticsearch) (2.1.0)\n",
            "Requirement already satisfied: certifi in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from elastic-transport<9,>=8->elasticsearch) (2023.11.17)\n",
            "Requirement already satisfied: filelock in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.14.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: sympy in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence_transformers) (12.3.101)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.0)\n",
            "Requirement already satisfied: click in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from torchvision->sentence_transformers) (10.1.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# install packages\n",
        "%pip install -U langchain elasticsearch tiktoken sentence_transformers llama-cpp-python wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import modules\n",
        "from getpass import getpass\n",
        "from langchain.vectorstores import ElasticsearchStore\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from urllib.request import urlopen\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "import json\n",
        "import os\n",
        "import wget\n",
        "\n",
        "cwd = os.getcwd()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qtEOCsCLWCZp"
      },
      "source": [
        "## Connect to Elasticsearch\n",
        "\n",
        "[ElasticsearchStore](https://api.python.langchain.com/en/latest/Vectorstores/langchain.Vectorstores.elasticsearch.ElasticsearchStore.html)를 사용하여 Elastic Cloud 배포에 연결하겠습니다. 이렇게 하면 데이터를 쉽게 생성하고 색인화하는 데 도움이 됩니다. \n",
        "\n",
        "ElasticsearchStore 인스턴스에서 임베딩을 [HuggingFaceEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.huggingface.HuggingFaceEmbeddings.html)로 설정하여 해당 텍스트와 Elasticsearch 인덱스 이름을 임베드합니다. 임베딩 모델은 [intfloat/multilingual-e5-base](https://huggingface.co/intfloat/multilingual-e5-base) 모델을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a-t1mglib54F"
      },
      "outputs": [],
      "source": [
        "# set elastic client info\n",
        "ES_URL = \"https://localhost:9200\" #input('Elasticsearch URL(ex:https://127.0.0.1:9200): ')\n",
        "ES_USER = \"elastic\" \n",
        "ES_USER_PASSWORD = \"elastic\" #getpass('elastic user PW: ')\n",
        "CERT_PATH = \"/home/wonseop/es/8.11.1/kibana-8.11.1/data/ca_1700913435542.crt\" #input('Elasticsearch pem 파일 경로: ')\n",
        "# pem 생성 방법: https://cdax.ch/2022/02/20/elasticsearch-python-workshop-1-the-basics/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "client = Elasticsearch(\n",
        "    ES_URL,\n",
        "    basic_auth=(ES_USER, ES_USER_PASSWORD),\n",
        "    ca_certs=CERT_PATH\n",
        ")\n",
        "\n",
        "if client.indices.exists(index=\"workplace_index\"):\n",
        "    client.indices.delete(index=\"workplace_index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "cwd = os.getcwd()\n",
        "\n",
        "if os.path.isdir(cwd + \"/models\"):\n",
        "    pass\n",
        "else:\n",
        "    os.mkdir(cwd + \"/models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'multilingual-e5-base' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "os.chdir(cwd + \"/models\")\n",
        "\n",
        "try :\n",
        "    os.system(\"git lfs install & git clone https://huggingface.co/intfloat/multilingual-e5-base\")\n",
        "except:\n",
        "    print('이미 모델이 존재합니다.')\n",
        "\n",
        "os.chdir(cwd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/wonseop/miniconda3/envs/genai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=cwd + \"/models/multilingual-e5-base\", model_kwargs = {'device': 'cpu'} )\n",
        "\n",
        "vector_store = ElasticsearchStore(\n",
        "    es_connection = client,\n",
        "    index_name=\"workplace_index\",\n",
        "    embedding=embeddings\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download the dataset \n",
        "\n",
        "샘플 데이터 세트를 다운로드하고 문서를 역직렬화해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "J8-93TiJsNyK"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/example-apps/chatbot-rag-app/data/data.json\"\n",
        "\n",
        "response = urlopen(url)\n",
        "\n",
        "workplace_docs = json.loads(response.read())\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "p0cQFDl1b9v4"
      },
      "source": [
        "### Split Documents into Passages\n",
        "\n",
        "검색 특이성(retrieval specificity)을 향상하고 최종 질문 답변 프롬프트의 컨텍스트 창 내에서 여러 구절을 제공할 수 있도록 문서를 구절로 청크할 것입니다.\n",
        "\n",
        "여기서는 400개의 토큰이 겹치는 800개의 토큰 구절로 문서를 청크합니다.\n",
        "\n",
        "여기서는 간단한 스플리터를 사용하고 있지만 Langchain은 컨텍스트 손실 가능성을 줄이기 위해 고급 스플리터를 제공합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dbHEoTF6vBXE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Created a chunk of size 866, which is longer than the specified 800\n",
            "Created a chunk of size 1120, which is longer than the specified 800\n"
          ]
        }
      ],
      "source": [
        "metadata = []\n",
        "content = []\n",
        "\n",
        "for doc in workplace_docs:\n",
        "  content.append(doc[\"content\"])\n",
        "  metadata.append({\n",
        "      \"name\": doc[\"name\"],\n",
        "      \"summary\": doc[\"summary\"]\n",
        "  })\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=400)\n",
        "docs = text_splitter.create_documents(content, metadatas=metadata)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RmCUl0hxW4lG"
      },
      "source": [
        "## Index data into elasticsearch\n",
        "\n",
        "이제 각 문서를 800개의 청크 크기로 분할했으므로 이제 [ElasticsearchStore.from_documents](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.elasticsearch.ElasticsearchStore.html#langchain.vectorstores.elasticsearch.ElasticsearchStore.from_documents)를 사용하여 Elasticsearch에 데이터를 인덱싱하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents = vector_store.from_documents(\n",
        "    docs, \n",
        "    embeddings, \n",
        "    es_connection=client, \n",
        "    index_name=\"workplace_index\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rXJH_MiWejv7"
      },
      "source": [
        "## Asking a question\n",
        "\n",
        "이제 Elasticsearch에 구절이 저장되었으므로 관련 구절을 얻기 위해 질문을 할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wget\n",
        "\n",
        "if os.path.isfile(cwd + \"/models/openbuddy-mistral-7b-v13.Q4_K_M.gguf\"):\n",
        "    pass\n",
        "else:\n",
        "    wget.download(\"https://huggingface.co/TheBloke/openbuddy-mistral-7B-v13-GGUF/resolve/main/openbuddy-mistral-7b-v13.Q4_K_M.gguf\", out=cwd + \"/models/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OobeBT6rek7Q",
        "outputId": "ba7b3a7a-253e-4e7f-83b9-cec07ebdac09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /home/wonseop/Projects/es-lab-kr/notebooks/generative-ai/models/openbuddy-mistral-7b-v13.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 36608,     1,     1 ]\n",
            "llama_model_loader: - tensor    1:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor    2:              blk.0.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor    3:              blk.0.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor    4:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor    6:              blk.0.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor    7:            blk.0.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   10:              blk.1.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   11:              blk.1.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   12:              blk.1.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   13:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   15:              blk.1.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   16:            blk.1.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   19:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   20:              blk.2.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   21:              blk.2.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   22:         blk.2.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   24:              blk.2.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   25:            blk.2.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   28:              blk.3.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   29:              blk.3.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   30:              blk.3.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   33:              blk.3.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   34:            blk.3.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   37:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   38:              blk.4.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   39:              blk.4.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   40:         blk.4.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   42:              blk.4.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   43:            blk.4.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   46:              blk.5.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   47:              blk.5.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   48:              blk.5.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   49:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   51:              blk.5.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   52:            blk.5.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   55:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   56:              blk.6.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   57:              blk.6.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   58:         blk.6.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   60:              blk.6.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   61:            blk.6.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   64:              blk.7.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   65:              blk.7.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   66:              blk.7.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   67:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   69:              blk.7.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   70:            blk.7.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   73:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   74:              blk.8.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   75:              blk.8.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   76:         blk.8.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   78:              blk.8.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   79:            blk.8.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   82:              blk.9.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   83:              blk.9.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   84:              blk.9.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   85:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   87:              blk.9.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   88:            blk.9.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   91:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   92:             blk.10.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   93:             blk.10.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   94:        blk.10.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   96:             blk.10.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   97:           blk.10.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  100:             blk.11.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  101:             blk.11.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  102:             blk.11.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  103:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  105:             blk.11.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  106:           blk.11.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  109:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  110:             blk.12.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  111:             blk.12.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  112:        blk.12.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  114:             blk.12.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  115:           blk.12.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  118:             blk.13.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  119:             blk.13.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  120:             blk.13.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  121:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  123:             blk.13.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  124:           blk.13.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  127:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  128:             blk.14.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  129:             blk.14.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  130:        blk.14.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  132:             blk.14.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  133:           blk.14.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  136:             blk.15.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  137:             blk.15.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  138:             blk.15.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  139:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  141:             blk.15.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  142:           blk.15.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  145:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  146:             blk.16.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  147:             blk.16.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  148:        blk.16.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  150:             blk.16.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  151:           blk.16.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  154:             blk.17.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  155:             blk.17.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  156:             blk.17.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  157:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  159:             blk.17.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  160:           blk.17.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  163:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  164:             blk.18.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  165:             blk.18.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  166:        blk.18.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  168:             blk.18.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  169:           blk.18.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  172:             blk.19.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  173:             blk.19.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  174:             blk.19.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  175:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  177:             blk.19.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  178:           blk.19.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  181:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  182:             blk.20.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  183:             blk.20.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  184:        blk.20.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  186:             blk.20.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  187:           blk.20.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  190:             blk.21.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  191:             blk.21.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  192:             blk.21.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  193:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  195:             blk.21.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  196:           blk.21.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  199:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  200:             blk.22.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  201:             blk.22.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  202:        blk.22.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  204:             blk.22.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  205:           blk.22.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  208:             blk.23.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  209:             blk.23.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  210:             blk.23.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  211:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  213:             blk.23.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  214:           blk.23.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  217:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  218:             blk.24.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  219:             blk.24.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  220:        blk.24.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  221:           blk.24.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  222:             blk.24.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  223:           blk.24.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  226:             blk.25.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  227:             blk.25.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  228:             blk.25.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  229:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  230:           blk.25.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  231:             blk.25.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  232:           blk.25.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  235:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  236:             blk.26.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  237:             blk.26.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  238:        blk.26.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  239:           blk.26.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  240:             blk.26.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  241:           blk.26.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  244:             blk.27.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  245:             blk.27.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  246:             blk.27.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  247:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  248:           blk.27.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  249:             blk.27.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  250:           blk.27.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  253:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  254:             blk.28.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  255:             blk.28.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  256:        blk.28.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  257:           blk.28.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  258:             blk.28.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  259:           blk.28.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  262:             blk.29.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  263:             blk.29.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  264:             blk.29.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  265:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  266:           blk.29.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  267:             blk.29.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  268:           blk.29.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  271:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  272:             blk.30.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  273:             blk.30.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  275:           blk.30.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  276:             blk.30.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  277:           blk.30.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  280:             blk.31.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  281:             blk.31.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  282:             blk.31.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  283:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  284:           blk.31.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  285:             blk.31.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  286:           blk.31.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  290:                    output.weight q6_K     [  4096, 36608,     1,     1 ]\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = openbuddy_openbuddy-mistral-7b-v13\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,36608]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,36608]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,36608]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: mismatch in special tokens definition ( 361/36608 vs 259/36608 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 36608\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = mostly Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.28 B\n",
            "llm_load_print_meta: model size       = 4.09 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name   = openbuddy_openbuddy-mistral-7b-v13\n",
            "llm_load_print_meta: BOS token = 1 '<s>'\n",
            "llm_load_print_meta: EOS token = 2 '</s>'\n",
            "llm_load_print_meta: UNK token = 0 '<unk>'\n",
            "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
            "llm_load_tensors: mem required  = 4190.36 MiB\n",
            "..............................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_new_context_with_model: kv self size  =  512.00 MiB\n",
            "llama_build_graph: non-view tensors processed: 740/740\n",
            "llama_new_context_with_model: compute buffer total size = 3.63 MiB\n",
            "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "retriever = vector_store.as_retriever()\n",
        "\n",
        "n_gpu_layers = None  # Metal set to 1 is enough.\n",
        "n_batch = 1  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    # https://huggingface.co/TheBloke/openbuddy-mistral-7B-v13-GGUF\n",
        "    model_path = cwd + \"/models/openbuddy-mistral-7b-v13.Q4_K_M.gguf\",\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    n_ctx=4096,\n",
        "\n",
        "    # https://www.reddit.com/r/LocalLLaMA/comments/1343bgz/what_model_parameters_is_everyone_using/\n",
        "    temperature=0.75,\n",
        "    top_k=1,\n",
        "    top_p=1,\n",
        "\n",
        "    # max_tokens=2048,\n",
        "    verbose=True,\n",
        "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
        "    callback_manager=callback_manager,\n",
        ")\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "ans = qa({\"query\": \"What does NASA stand for?\"})\n",
        "\n",
        "print(\"---- Answer ----\")\n",
        "print(ans[\"result\"])\n",
        "print(\"---- Sources ----\")\n",
        "for doc in ans[\"source_documents\"]:\n",
        "  print(doc.metadata[\"name\"])\n",
        "  print(doc.page_content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
