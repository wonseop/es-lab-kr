{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local LLM(Falcon 70B) 및 LangChain을 사용한 개인 챗봇\n",
    "\n",
    "https://www.mlexpert.io/prompt-engineering/chatbot-with-local-llm-using-langchain\n",
    "\n",
    "단일 GPU에서 로컬 LLM을 사용하여 ChatGPT와 같은 성능의 비공개 챗봇을 구축할 수 있습니까?\n",
    "\n",
    "이 자습서에서는 Falcon 70B를 사용합니다1 LangChain을 사용하여 대화 메모리를 유지하는 챗봇을 구축합니다. 단일 T4 GPU를 활용하고 8 비트 (~ 6 토큰 / 초)로 모델을 로드하여 적절한 성능을 얻을 수 있습니다. 또한 다음과 같이 출력 품질과 속도를 개선하는 기술을 살펴봅니다.\n",
    "\n",
    "중지 기준 : LLM \"rambling\"의 시작을 감지하고 생성을 중지합니다.\n",
    "정리 출력 : 때때로 LLM이 이상하거나 추가 토큰을 출력하는 경우 출력에서 토큰을 지우는 방법을 보여 드리겠습니다\n",
    "채팅 기록 저장: LLM이 대화 기록을 기억할 수 있도록 메모리를 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설치\n",
    "\n",
    "-----\n",
    "\n",
    "필요한 종속성을 설치하는 것부터 시작하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqqq pip --progress-bar off\n",
    "!pip install -qqq bitsandbytes==0.40.0 --progress-bar off\n",
    "!pip install -qqq torch==2.0.1 --progress-bar off\n",
    "!pip install -qqq transformers==4.30.0 --progress-bar off\n",
    "!pip install -qqq accelerate==0.21.0 --progress-bar off\n",
    "!pip install -qqq xformers==0.0.20 --progress-bar off\n",
    "!pip install -qqq einops==0.6.1 --progress-bar off\n",
    "!pip install -qqq langchain==0.0.233 --progress-bar off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요한 import 목록은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "from typing import List\n",
    " \n",
    "import torch\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.schema import BaseOutputParser\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    pipeline,\n",
    ")\n",
    " \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 로드\n",
    "\n",
    "직접 로컬에 있는 모델을 로드할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "MODEL_NAME = cwd + \"/models/falcon-7b-instruct\"\n",
    " \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, trust_remote_code=True, load_in_8bit=True, device_map=\"auto\"\n",
    ")\n",
    "model = model.eval()\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "es",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
