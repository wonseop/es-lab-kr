{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 키버트(KeyBERT)를 이용한 키워드 추출\n",
    "\n",
    "단순 통계적 특성 뿐만아니라 의미적 유사성을 고려하기 위해서 여기서는 SBERT 임베딩을 활용하여 사용하기 쉬운 키워드 추출 알고리즘인 KeyBERT를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keybert in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: sentence_transformers in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from keybert) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from keybert) (1.23.5)\n",
      "Requirement already satisfied: rich>=10.4.0 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from keybert) (13.5.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from sentence_transformers) (4.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from sentence_transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: torchvision in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from sentence_transformers) (0.14.1)\n",
      "Requirement already satisfied: scipy in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from sentence_transformers) (1.11.2)\n",
      "Requirement already satisfied: nltk in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from sentence_transformers) (0.16.4)\n",
      "Requirement already satisfied: filelock in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n",
      "Requirement already satisfied: fsspec in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from rich>=10.4.0->keybert) (2.16.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: click in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from nltk->sentence_transformers) (8.1.7)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from torchvision->sentence_transformers) (10.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\tools\\miniconda3\\envs\\vector\\lib\\site-packages)\n",
      "DEPRECATION: pytorch-lightning 1.6.1 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "%pip install keybert sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "try :\n",
    "    os.mkdir(cwd + \"/models\") \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(cwd + \"/models\")\n",
    "\n",
    "try :\n",
    "    os.system(\"git clone https://huggingface.co/lcw99/t5-base-korean-text-summary\")\n",
    "except:\n",
    "    print('이미 모델이 존재합니다.')\n",
    "\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KeyBert[https://github.com/MaartenGr/KeyBERT#embeddings] 모듈을 사용하여 키워드를 쉽게 추출할 수 있습니다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = SentenceTransformer(cwd + '/models/paraphrase-multilingual-MiniLM-L12-v2', device=device)\n",
    "kw_model = KeyBERT(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "서울 지진 대피소에 대한 데이터 분석을 위해서는 어떤 종류의 데이터가 필요할까요? 예를 들어, 서울시의 지진 대피소 위치, 수용 가능 인원, 대피소 내부 시설물, 대피소 이용 현황 등의 정보가 필요할 것입니다. \n",
    "지진 대피소 위치 분석 예시: 지진 대피소 위치는 서울시 공공데이터 포털에서 제공하는 \"서울시 지진대피소 안내\" 데이터를 사용할 수 있습니다. 이 데이터셋에는 지진 대피소 명칭, 위치(주소), 좌표, 수용 인원, 관리 기관 등의 항목이 포함되어 있습니다. \n",
    "이를 바탕으로 대피소 위치를 지도에 시각화하여 지진 발생 시 대피소가 필요한 지역을 파악할 수 있습니다. 대피소 이용 현황 분석 예시: 대피소 이용 현황은 서울시에서 제공하는 \"서울시 재난정보 실시간 수집 및 제공 서비스\" 데이터를 사용할 수 있습니다. \n",
    "이 데이터셋에는 대피소 이용 현황(대피소 이용 가능 여부, 이용 중인 인원 수), 지진 발생 시 대피소 이용 현황 등의 정보가 포함되어 있습니다. 이를 바탕으로 대피소 이용 현황을 분석하여 인원이 많은 대피소를 파악하거나, 대피소 이용 가능 여부 등을 파악할 수 있습니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('지진대피소', 0.4972), ('지진', 0.4423), ('공공데이터', 0.4249), ('서울', 0.4239), ('서울시에서', 0.3922)]\n",
      "[('서울시 지진대피소', 0.6382), ('서울시의 지진', 0.6199), ('서울 지진', 0.6039), ('데이터셋에는 지진', 0.5942), ('서울시 공공데이터', 0.561)]\n"
     ]
    }
   ],
   "source": [
    "keywords = kw_model.extract_keywords(text)\n",
    "print(keywords)\n",
    "\n",
    "keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(2, 2))\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5개의 키워드가 출력되는데, 몇몇 키워드는 의미가 좀 비슷해보입니다.  \n",
    "비슷한 의미의 키워드들이 반환되는 데는 이 키워드들이 문서를 가장 잘 나타내고 있기 때문입니다.  \n",
    "다양한 키워드를 출력하고 싶다면 선정의 정확성과 다양성 사이의 조절이 필요합니다.\n",
    "\n",
    "KeyBert 모듈은 다음과 같은 연관 키워드 조합 관련 알고리즘을 지원합니다.\n",
    "- Max Sum Similarity\n",
    "- Maximal Marginal Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Sum Similarity\n",
    " \n",
    "Max Sum Similarity 알고리즘은 후보(candidate)들 간의 유사성을 최소화 하면서 원문과 후보간 유사성을 극대화합니다.   \n",
    "즉, 코사인 유사도에 기반하여 원문과 유사한 상위 nr_candidates 개의 단어를 선정하고  \n",
    "단어간 덜 유사한 키워드 간 조합을 계산합니다.   \n",
    "낮은 nr_candidates를 설정하면 결과는 출력된 키워드 5개는 유사하며,  \n",
    "상대적으로 높은 nr_candidates는 더 다양한 키워드 5개를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('예를', 0.1343),\n",
       " ('시설물', 0.2401),\n",
       " ('서울', 0.4239),\n",
       " ('공공데이터', 0.4249),\n",
       " ('지진', 0.4423)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1),\n",
    "                              use_maxsum=True, nr_candidates=30, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('실시간 수집', 0.2795),\n",
       " ('위치 주소', 0.3053),\n",
       " ('현황은 서울시에서', 0.3489),\n",
       " ('데이터 분석을', 0.3763),\n",
       " ('인원 지진', 0.4429)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model.extract_keywords(text, keyphrase_ngram_range=(2, 2),\n",
    "                              use_maxsum=True, nr_candidates=30, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMR(Maximal Marginal Relevance)\n",
    "\n",
    "문서와 코사인 유사도가 가장 유사한 키워드를 선택한 후,   \n",
    "문서와는 유사하나 이미 선택된 키워드와 유사하지 않은 새로운 후보를 반복적으로 선택하는 알고리즘 입니다.\n",
    "\n",
    "diversity 변수는 0~1사이의 소수이며,   \n",
    "낮은 diversity 값을 설정한다면, 결과는 기존의 코사인 유사도만 사용한 것과 매우 유사하고,   \n",
    "상대적으로 높은 diversity값은 더 다양한 키워드를 추출합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('지진대피소', 0.4972),\n",
       " ('공공데이터', 0.4249),\n",
       " ('서울', 0.4239),\n",
       " ('재난정보', 0.3739),\n",
       " ('시설물', 0.2401)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1),\n",
    "                              use_mmr=True, diversity=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('서울시 지진대피소', 0.6382),\n",
       " ('서울시 공공데이터', 0.561),\n",
       " ('재난정보 실시간', 0.4145),\n",
       " ('종류의 데이터가', 0.4055),\n",
       " ('위치 주소', 0.3053)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model.extract_keywords(text, keyphrase_ngram_range=(2, 2),\n",
    "                              use_mmr=True, diversity=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
