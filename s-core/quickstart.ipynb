{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG using local models\n",
    "\n",
    "\n",
    "\n",
    "https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (0.0.1)\n",
      "Requirement already satisfied: tiktoken in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: langchain in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (0.0.319)\n",
      "Requirement already satisfied: fake_useragent in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: elasticsearch in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (8.10.1)\n",
      "Requirement already satisfied: eland==8.10.1 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from eland[pytorch]==8.10.1) (8.10.1)\n",
      "Requirement already satisfied: pandas<2,>=1.5 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from eland==8.10.1->eland[pytorch]==8.10.1) (1.5.3)\n",
      "Requirement already satisfied: matplotlib>=3.6 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from eland==8.10.1->eland[pytorch]==8.10.1) (3.8.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.2.0 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from eland==8.10.1->eland[pytorch]==8.10.1) (1.23.5)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from eland==8.10.1->eland[pytorch]==8.10.1) (23.2)\n",
      "Requirement already satisfied: torch<2.0,>=1.13.1 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from eland[pytorch]==8.10.1) (1.13.1)\n",
      "Requirement already satisfied: sentence-transformers<=2.2.2,>=2.1.0 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from eland[pytorch]==8.10.1) (2.2.2)\n",
      "Requirement already satisfied: transformers<=4.33.2,>=4.31.0 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from transformers[torch]<=4.33.2,>=4.31.0; extra == \"pytorch\"->eland[pytorch]==8.10.1) (4.33.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from langchain) (2.0.22)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from langchain) (3.8.6)\n",
      "Requirement already satisfied: anyio<4.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from langchain) (0.5.14)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from langchain) (0.0.47)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from langchain) (2.4.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: elastic-transport<9,>=8 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from elasticsearch) (8.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from anyio<4.0->langchain) (1.1.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2,>=1.26.2 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from elastic-transport<9,>=8->elasticsearch) (1.26.18)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from elastic-transport<9,>=8->elasticsearch) (2023.7.22)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from matplotlib>=3.6->eland==8.10.1->eland[pytorch]==8.10.1) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from matplotlib>=3.6->eland==8.10.1->eland[pytorch]==8.10.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from matplotlib>=3.6->eland==8.10.1->eland[pytorch]==8.10.1) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from matplotlib>=3.6->eland==8.10.1->eland[pytorch]==8.10.1) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from matplotlib>=3.6->eland==8.10.1->eland[pytorch]==8.10.1) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from matplotlib>=3.6->eland==8.10.1->eland[pytorch]==8.10.1) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from matplotlib>=3.6->eland==8.10.1->eland[pytorch]==8.10.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from pandas<2,>=1.5->eland==8.10.1->eland[pytorch]==8.10.1) (2023.3.post1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch]==8.10.1) (0.14.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch]==8.10.1) (1.3.1)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch]==8.10.1) (1.11.3)\n",
      "Requirement already satisfied: sentencepiece in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch]==8.10.1) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch]==8.10.1) (0.17.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from transformers<=4.33.2,>=4.31.0->transformers[torch]<=4.33.2,>=4.31.0; extra == \"pytorch\"->eland[pytorch]==8.10.1) (3.12.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from transformers<=4.33.2,>=4.31.0->transformers[torch]<=4.33.2,>=4.31.0; extra == \"pytorch\"->eland[pytorch]==8.10.1) (0.12.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from transformers<=4.33.2,>=4.31.0->transformers[torch]<=4.33.2,>=4.31.0; extra == \"pytorch\"->eland[pytorch]==8.10.1) (0.4.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from transformers[torch]<=4.33.2,>=4.31.0; extra == \"pytorch\"->eland[pytorch]==8.10.1) (0.23.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from accelerate>=0.20.3->transformers[torch]<=4.33.2,>=4.31.0; extra == \"pytorch\"->eland[pytorch]==8.10.1) (5.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch]==8.10.1) (2023.9.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.6->eland==8.10.1->eland[pytorch]==8.10.1) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from scikit-learn->sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch]==8.10.1) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4 tiktoken nltk langchain fake_useragent elasticsearch eland[pytorch]==8.10.1 --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from langchain.vectorstores import ElasticsearchStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ES_URL = input('Elasticsearch URL: ')\n",
    "ES_URL = \"https://127.0.0.1:9200\" # input('Elasticsearch URL(ex:https://127.0.0.1:9200)')\n",
    "ES_USER = \"elastic\" \n",
    "ES_USER_PASSWORD = \"elastic\" # getpass('elastic user PW: ')\n",
    "CERT_PATH = \"D:\\es\\8.10.4\\kibana-8.10.4\\data\\ca_1698029751849.crt\" # input('Elasticsearch pem 파일 경로: ')\n",
    "# pem 생성 방법: https://cdax.ch/2022/02/20/elasticsearch-python-workshop-1-the-basics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "client = Elasticsearch(\n",
    "    ES_URL,\n",
    "    basic_auth=(ES_USER, ES_USER_PASSWORD),\n",
    "    ca_certs=CERT_PATH,\n",
    "    timeout=60\n",
    ")\n",
    "\n",
    "if client.indices.exists(index=\"workplace_index\"):\n",
    "    client.indices.delete(index=\"workplace_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !eland_import_hub_model --url \"https://127.0.0.1:9200\" -u elastic -p elastic --ca-certs \"D:\\es\\8.10.2\\kibana-8.10.2\\data\\ca_1695716998259.crt\" --hub-model-id \"d:/Projects/es-lab-kr/s-core/models/distilbert-base-multilingual-cased\" --task-type text_embedding --start --clear-previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "try :\n",
    "    os.mkdir(cwd + \"/models\") \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(cwd + \"/models\")\n",
    "\n",
    "try :\n",
    "    os.system(\"git clone https://huggingface.co/distilbert-base-multilingual-cased\")\n",
    "except:\n",
    "    print('이미 모델이 존재합니다.')\n",
    "\n",
    "os.chdir(cwd)\n",
    "\n",
    "es_model_id = \"sentence-transformers_distilbert-base-multilingual-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\es-lab-kr\\s-core/models/distilbert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name d:\\Projects\\es-lab-kr\\s-core/models/distilbert-base-multilingual-cased. Creating a new one with MEAN pooling.\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from eland.ml import MLModel\n",
    "from eland.ml.pytorch import PyTorchModel\n",
    "from eland.common import es_version\n",
    "from eland.ml.pytorch.transformers import TransformerModel\n",
    "\n",
    "# 현재 경로 얻기\n",
    "cwd = os.getcwd()\n",
    "local_model_path = cwd + '/models/distilbert-base-multilingual-cased'\n",
    "\n",
    "print(local_model_path)\n",
    "\n",
    "# 모델 이름 및 작업 유형 설정\n",
    "#tm = TransformerModel(local_model_path, \"text_embedding\")\n",
    "es_cluster_version = es_version(client)\n",
    "tm = TransformerModel(\n",
    "    model_id=local_model_path, \n",
    "    task_type=\"text_embedding\", \n",
    "    es_version=es_cluster_version\n",
    ")\n",
    "\n",
    "# Elasticsearch용 modelID를 설정\n",
    "es_model_id = \"sentence-transformers_distilbert-base-multilingual-cased\"\n",
    "\n",
    "es_model = MLModel(client, es_model_id)\n",
    "\n",
    "if (es_model.exists_model() == False) :\n",
    "    tmp_path = \"models/sentence-transformers_distilbert-base-multilingual-cased\"\n",
    "    Path(tmp_path).mkdir(parents=True, exist_ok=True)\n",
    "    model_path, config, vocab_path = tm.save(tmp_path)\n",
    "\n",
    "    ptm = PyTorchModel(client, es_model_id)\n",
    "    ptm.import_model(\n",
    "        model_path=model_path,\n",
    "        config_path=None,\n",
    "        vocab_path=vocab_path,\n",
    "        config=config\n",
    "    ) \n",
    "    ptm.start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import ElasticsearchStore\n",
    "from langchain.embeddings.elasticsearch import ElasticsearchEmbeddings\n",
    "\n",
    "embeddings = ElasticsearchEmbeddings.from_es_connection(\n",
    "    es_connection=client,\n",
    "    model_id = es_model_id\n",
    ")\n",
    "\n",
    "vectorstore = ElasticsearchStore(\n",
    "    es_connection=client, \n",
    "    embedding=embeddings, \n",
    "    index_name= \"workplace_index\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.request import urlopen\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# import json\n",
    "\n",
    "# url = \"./data.json\"\n",
    "\n",
    "# response = urlopen(\"https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/example-apps/workplace-search/data/data.json\")\n",
    "\n",
    "# workplace_docs = json.loads(response.read())\n",
    "\n",
    "# workplace_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata = []\n",
    "# content = []\n",
    "\n",
    "# for doc in workplace_docs:\n",
    "#   content.append(doc[\"content\"])\n",
    "#   metadata.append({\n",
    "#       \"name\": doc[\"name\"],\n",
    "#       \"summary\": doc[\"summary\"],\n",
    "#       \"rolePermissions\":doc[\"rolePermissions\"],\n",
    "#   })\n",
    "\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)\n",
    "# docs = text_splitter.create_documents(content, metadatas=metadata)\n",
    "\n",
    "# metadata\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\urllib3\\connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy.score'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "Created a chunk of size 2828, which is longer than the specified 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic BlogSkip to main contentPlatformPlatform솔루션솔루션고객고객리소스리소스요금제요금제설명서설명서Language pickerDeutschEnglishEspañolFrançais日本語한국어简体中文PortuguêsSearchLogin무료 체험판 사용 시작영업팀에 문의하기Elasticsearch PlatformElasticsearch Platform이 제공하는 Observability, Security, Search 솔루션Elastic 개요ELK Stack검색, 분석, 데이터 수집, 시각화를 간편하게KibanaElasticsearch통합ELK Stack 개요Elastic Cloud원하는 클라우드 서비스 제공자에서 Elastic을 사용하여 중요한 답을 찾아보세요.Cloud 개요파트너영업팀에 문의하기Observability앱과 인프라 가시성을 통합하여 문제를 사전에 해결합니다.로그 모니터링애플리케이션 성능 모니터링인프라 모니터링가상 모니터링실제 사용자 모니터링Universal ProfilingAIOpsOpenTelemetryObservability 개요Security대규모 사이버 위협으로부터 신속하게 보호하고 조사하고 대응하세요.지속적 모니터링위협 헌팅조사 및 사고 대응자동화된 위협 보호Security 개요Search모든 클라우드에 걸쳐 검색 결과를 가속화하고 맞춤형 검색을 강화하세요.생성형 AI검색 애플리케이션전자 상거래웹사이트Workplace Search고객 지원Search 개요산업별공공 부문금융 서비스통신의료기술소매 및 전자 상거래제조 및 자동차모든 산업 보기솔루션별고객에게 미래에 대비할 수 있는 유연성, 속도, 규모를 제공합니다.ObservabilitySecuritySearch고객 스포트라이트Cisco가 어떻게 AI를 통해 검색 경험을 혁신하는지 알아보세요.자세히 알아보기RWE가 어떻게 재생 에너지 거래 사업을 지원하는지 알아보세요.자세히 알아보기Comcast가 어떻게 엔지니어링 속도를 높이고 혁신을 강화하는지 알아보세요.자세히 알아보기개발자코드, 포럼, 그룹과 관련된 모든 것을 알아보세요.커뮤니티포럼다운로드소통최신 기술 주제, 혁신, 뉴스로 최신 정보를 받아보세요.이벤트블로그학습역량을 강화하고 성공을 위한 기회를 마련하세요.시작하기Elastic 리소스컨설팅 서비스교육 및 인증Security Labs도움말어떤 주제이든 필요한 지원을 찾아보세요.문의하기지원 센터Elastic에서 어떤 일이 진행되고 있는지 확인데모 갤러리 보기자세히 알아보기Elasticsearch로 시작하세요지금 시청하기Elastic 8.9의 새로운 기능자세히 알아보기SearchPlatformElasticsearch PlatformElasticsearch Platform이 제공하는 Observability, Security, Search 솔루션Elastic 개요ELK Stack검색, 분석, 데이터 수집, 시각화를 간편하게KibanaElasticsearch통합ELK Stack 개요Elastic Cloud원하는 클라우드 서비스 제공자에서 Elastic을 사용하여 중요한 답을 찾아보세요.Cloud 개요파트너영업팀에 문의하기솔루션Observability앱과 인프라 가시성을 통합하여 문제를 사전에 해결합니다.로그 모니터링애플리케이션 성능 모니터링인프라 모니터링가상 모니터링실제 사용자 모니터링Universal ProfilingAIOpsOpenTelemetryObservability 개요Security대규모 사이버 위협으로부터 신속하게 보호하고 조사하고 대응하세요.지속적 모니터링위협 헌팅조사 및 사고 대응자동화된 위협 보호Security 개요Search모든 클라우드에 걸쳐 검색 결과를 가속화하고 맞춤형 검색을 강화하세요.생성형 AI검색 애플리케이션전자 상거래웹사이트Workplace Search고객 지원Search 개요산업별공공 부문금융 서비스통신의료기술소매 및 전자 상거래제조 및 자동차모든 산업 보기고객솔루션별고객에게 미래에 대비할 수 있는 유연성, 속도, 규모를 제공합니다.ObservabilitySecuritySearch고객 스포트라이트Cisco가 어떻게 AI를 통해 검색 경험을 혁신하는지 알아보세요.자세히 알아보기RWE가 어떻게 재생 에너지 거래 사업을 지원하는지 알아보세요.자세히 알아보기Comcast가 어떻게 엔지니어링 속도를 높이고 혁신을 강화하는지 알아보세요.자세히 알아보기리소스개발자코드, 포럼, 그룹과 관련된 모든 것을 알아보세요.커뮤니티포럼다운로드소통최신 기술 주제, 혁신, 뉴스로 최신 정보를 받아보세요.이벤트블로그학습역량을 강화하고 성공을 위한 기회를 마련하세요.시작하기Elastic 리소스컨설팅 서비스교육 및 인증Security Labs도움말어떤 주제이든 필요한 지원을 찾아보세요.문의하기지원 센터Elastic에서 어떤 일이 진행되고 있는지 확인데모 갤러리 보기자세히 알아보기Elasticsearch로 시작하세요지금 시청하기Elastic 8.9의 새로운 기능자세히 알아보기요금제설명서Language picker 언어DeutschEnglishEspañolFrançais日本語한국어简体中文PortuguêsLogin 로그인무료 체험판 사용 시작영업팀에 문의하기블로그솔루션Stack + Cloud기술 주제소식고객인사이트AI 혁신을 위한 고급 검색, Elasticsearch Relevance Engine™ 소개ByMatt Riley2023년 5월 23일목차트위터에서 공유하기링크드인에서 공유하기페이스북에서 공유하기이메일로 공유하기인쇄하기오늘은 매우 정확한 AI 검색 애플리케이션을 만들기 위한 새로운 기능인 Elasticsearch Relevance Engine™(ESRE™)을 소개하겠습니다', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}),\n",
       " Document(page_content='ESRE는 검색 분야를 선도하는 Elastic의 리더십과 2년에 걸친 머신 러닝 연구 및 개발을 기반으로 구축되었습니다\\\\. Elasticsearch Relevance Engine은 AI의 장점과 Elastic의 텍스트 검색을 결합합니다\\\\. ESRE는 개발자들에게 정교한 검색 알고리즘의 전체 제품군과 대규모 언어 모델(LLM)과 통합할 수 있는 기능을 제공합니다', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}),\n",
       " Document(page_content='더 좋은 점은 Elastic 커뮤니티에서 이미 신뢰하고 있는 간단한 통합 API를 통해 액세스할 수 있기 때문에 전 세계 개발자들이 즉시 이를 사용하여 검색 정확도를 높일 수 있다는 것입니다.Elasticsearch Relevance Engine의 구성 가능한 기능은 다음과 같은 방법으로 정확도를 개선하는 데 도움이 될 수 있습니다.하이브리드 검색의 중요한 구성 요소인 BM25f 등 고급 정확도 순위 지정 기능 적용Elastic의 벡터 데이터베이스를 사용하여 밀집 임베딩 생성, 저장 및 검색다양한 자연어 처리(NLP) 작업 및 모델을 사용하여 텍스트 처리개발자가 비즈니스 관련 컨텍스트에 맞게 Elastic에서 자체 트랜스포머 모델을 관리 및 사용하도록 허용API를 통해 OpenAI의 GPT-3 및 4와 같은 서드파티 트랜스포머 모델과 통합하여 Elasticsearch 배포 내에 통합된 고객의 데이터 저장소를 기반으로 콘텐츠의 직관적인 요약을 검색Elastic의 즉시 사용 가능한 Learned Sparse Encoder 모델을 사용하여 모델을 훈련하거나 유지관리하지 않고 ML 지원 검색을 활성화하여 다양한 도메인에 걸쳐 매우 정확한 시맨틱 검색 제공개발자가 AI 검색 엔진을 자연어 및 키워드 쿼리 유형의 고유한 조합에 최적화하도록 제어할 수 있는 하이브리드 순위 지정 방법인 상호 순위 결합(RRF)을 사용하여 희소 검색과 밀접 검색을 손쉽게 결합LangChain과 같은 서드파티 도구와 통합하여 정교한 데이터 파이프라인 및 생성형 AI 애플리케이션 구축 지원검색의 진화는 정확도를 개선하고 검색 애플리케이션과 상호 작용하는 방식을 개선해야 하는 지속적인 필요성에 의해 주도되어 왔습니다\\\\. 매우 정확한 검색 결과는 검색 앱에 대한 사용자 참여를 높이고 수익과 생산성 모두에 상당한 다운스트림 영향을 미칠 수 있습니다\\\\. LLM 및 생성형 AI의 새로운 세계에서 검색은 한 단계 더 나아갈 수 있습니다', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}),\n",
       " Document(page_content='즉, 사용자의 의도를 이해하여 이전에는 볼 수 없었던 구체적인 응답을 제공할 수 있습니다.특히, 모든 검색의 발전은 새로운 기술과 변화하는 사용자 행동으로 인해 제기된 새로운 문제를 해결하는 동시에 더 나은 정확도를 제공합니다\\\\. 시맨틱 검색을 제공하기 위해 키워드 검색을 확장하든 비디오 및 이미지에 대한 새로운 검색 양식을 활성화하든, 새로운 기술에는 검색 사용자에게 더 나은 경험을 제공하기 위한 고유한 도구가 필요합니다\\\\. 마찬가지로 오늘날의 인공 지능 세계에서는 고객이 테스트한 검증된 기능을 갖춘 기술 스택에 구축된 확장성이 뛰어난 새로운 개발자 툴킷이 필요합니다.생성형 AI의 추진력과 더불어 ChatGPT와 같은 기술 채택이 증가하고, 대규모 언어 모델 기능에 대한 인식이 높아지면서 개발자는 애플리케이션을 개선할 수 있는 기술을 실험하고 싶어 합니다\\\\. Elasticsearch Relevance Engine은 생성형 AI 세계에서 새로운 기능의 시대를 열었으며 모든 개발자 팀이 즉시 사용할 수 있는 강력한 도구를 제공합니다.Elasticsearch Relevance Engine은 Elastic Cloud에서 지금 제공되고 있습니다\\\\. 최신 릴리즈의 모든 새로운 기능들은 유일한 호스트형 Elasticsearch 제품인 Elastic Cloud에서만 이용하실 수 있습니다', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}),\n",
       " Document(page_content='또한 Elastic Stack과 클라우드 오케스트레이션 제품들, Elastic Cloud Enterprise와 Kubernetes의 Elastic Cloud를 다운로드하셔서 자체 관리형 환경을 만드셔도 됩니다.Elasticsearch Relevance Engine™에 대해 자세히 알고 싶으신가요? 다음 기술 블로그를 확인하세요.ChatGPT와 ElasticsearchElastic Learned Sparse Encoder 블로그Elastic에서 머신 러닝 모델에 액세스LangChain 및 Elasticsearch를 사용한 개인정보 보호 우선 AI 검색생성형 AI 모델의 한계 극복Elasticsearch Relevance Engine™은 개발자가 빠르게 발전하고 생성형 AI를 비롯하여 자연어 검색의 이러한 문제를 해결할 수 있도록 지원합니다.엔터프라이즈 데이터/컨텍스트 인식: 모델에 특정 도메인과 관련된 충분한 내부 지식이 없을 수 있습니다\\\\. 이는 모델 훈련에 사용된 데이터 세트에 따라 달라집니다\\\\. 기업이 LLM에서 생성하는 데이터와 콘텐츠를 맞춤화하기 위해서는 모델이 더 정확하고 비즈니스에 특화된 정보를 배울 수 있도록 모델에 독점 데이터를 제공하는 방법이 필요합니다.탁월한 정확도: Elasticsearch Relevance Engine은 시맨틱 검색을 사용하여 컨텍스트를 검색하기 위해 벡터 임베딩을 생성하고 저장하는 것처럼 간단하게 프라이빗 소스의 데이터를 통합합니다\\\\. 벡터 임베딩은 단어, 구문 또는 문서의 숫자 표현으로, LLM이 단어의 의미와 해당 관계를 이해하는 데 도움이 됩니다\\\\. 이러한 임베딩은 속도와 확장성 측면에서 트랜스포머 모델의 출력을 향상시킵니다\\\\. ESRE는 또한 개발자가 자체 트랜스포머 모델을 Elastic으로 가져오거나 서드파티 모델과 통합할 수 있도록 합니다.Elastic은 또한 최신 상호 작용 모델의 출현으로 서드파티 데이터 세트에 대한 광범위한 훈련이나 미세 조정 없이도 이를 즉시 제공할 수 있다는 것을 깨달았습니다', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}),\n",
       " Document(page_content='모든 개발 팀이 머신 러닝 모델을 훈련 및 유지관리하거나 확장성, 성능 및 속도에 대한 절충을 이해할 수 있는 리소스나 전문 지식을 보유하고 있는 것은 아니기 때문에 Elasticsearch Relevance Engine에는 다양한 도메인에 걸친 시맨틱 검색을 위해 구축된 검색 모델인 Elastic Learned Sparse Encoder도 포함되어 있습니다\\\\. 이 모델은 희소 임베딩을 기존의 키워드 기반 BM25 검색과 결합하여 하이브리드 검색을 위해 사용하기 쉬운 상호 순위 결합(RRF) 점수를 확보합니다\\\\. 개발자들은 ESRE를 통해 사용 첫날부터 머신 러닝 지원 정확도 및 하이브리드 검색 기술을 활용할 수 있습니다.개인정보 보호 및 보안: 데이터 개인정보 보호는 기업이 네트워크를 통해 그리고 구성 요소 간에 독점 데이터를 사용하고 안전하게 전달하는 방법에서 핵심적인 역할을 합니다\\\\. 이는 혁신적인 검색 경험을 구축하는 경우에도 마찬가지입니다.Elastic은 역할 기반 및 속성 기반 액세스 제어를 기본적으로 지원하여 채팅 및 질문 답변 애플리케이션에서도 데이터에 대한 액세스 권한이 있는 역할만 데이터를 볼 수 있도록 합니다\\\\. Elasticsearch는 권한이 있는 개인만 특정 문서에 액세스할 수 있도록 유지해야 하는 조직의 요구를 지원함으로써 모든 검색 애플리케이션에서 보편적인 개인정보 보호 및 액세스 제어를 유지할 수 있도록 돕습니다.개인정보 보호가 최대 관심사인 경우, 모든 데이터를 조직의 네트워크 내에 유지하는 것이 매우 중요할 뿐만 아니라 의무 사항이 될 수 있습니다\\\\. ESRE는 조직이 에어 갭 환경에서 배포를 구현할 수 있도록 허용하는 것부터 보안 네트워크에 대한 액세스를 지원하는 것까지 조직이 데이터를 안전하게 유지하는 데 필요한 도구를 제공합니다.규모와 비용: 데이터 볼륨과 필요한 컴퓨팅 성능 및 메모리는 많은 기업에서 대규모 언어 모델을 사용하지 못하는 장벽이 될 수 있습니다', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}),\n",
       " Document(page_content='그러나 챗봇과 같은 자체 생성형 AI 앱을 구축하려는 기업은 LLM과 프라이빗 데이터를 결합해야 합니다.Elasticsearch Relevance Engine은 정밀한 컨텍스트 창으로 정확도를 효율적으로 제공하는 엔진을 기업에 제공하여 번거로움이나 비용 없이 데이터 공간을 줄일 수 있도록 돕습니다.오래된 데이터: 모델은 데이터가 수집되는 시점에 고정됩니다\\\\. 따라서 생성형 AI 모델이 생성하는 콘텐츠와 데이터는 훈련에 사용된 데이터만큼만 최신입니다\\\\. LLM에서 결과를 적시에 도출하기 위해서는 기업 데이터 통합이 본질적으로 필요합니다.환각: 질문에 답하거나 모델과 대화할 때 모델은 신뢰할 수 있고 설득력 있게 들리는 사실을 만들어 낼 수 있지만 실제로는 사실이 아닌 예측입니다\\\\. 이것이 비즈니스 컨텍스트에서 유용한 모델을 만들려면 LLM을 상황에 맞는 맞춤형 지식으로 교육하는 것이 매우 중요한 또 다른 이유입니다.Elasticsearch Relevance Engine을 사용하면 개발자가 생성형 AI 모델의 컨텍스트 창을 통해 자체 데이터 저장소에 연결할 수 있습니다\\\\. 추가된 검색 결과는 프라이빗 소스 또는 전문 도메인의 최신 정보를 제공할 수 있으므로 질문을 받았을 때 모델의 소위 ‘파라메트릭’ 지식에만 의존할 때 보다 더 사실에 기반한 정보를 반환할 수 있습니다.벡터 데이터베이스로 강화Elasticsearch Relevance Engine에는 복원력을 갖춘 프로덕션 등급 벡터 데이터베이스가 내재화되어 있습니다\\\\. 이는 개발자에게 풍부한 시맨틱 검색 애플리케이션을 구축할 수 있는 기반을 제공합니다\\\\. 개발 팀은 Elastic의 플랫폼에서 밀집 벡터 검색을 사용하여 키워드나 동의어로 제한되지 않는 보다 직관적인 질문 답변을 생성할 수 있습니다\\\\. 이미지와 같은 비정형 데이터를 사용하여 멀티 모달 검색을 구축하고, 심지어 사용자 프로필을 모델링하고 일치 항목을 생성하여 제품 및 검색, 구직 또는 중매 애플리케이션에서 검색 결과를 개인에 맞게 제공할 수 있습니다', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}),\n",
       " Document(page_content='이러한 NLP 트랜스포머 모델은 또한 정서 분석, 명명된 엔터티 인식, 텍스트 분류와 같은 머신 러닝 작업을 지원합니다\\\\. Elastic의 벡터 데이터베이스를 사용하면 개발자가 실제 프로덕션 애플리케이션을 위해 확장성과 성능이 뛰어난 임베딩을 생성, 저장 및 쿼리할 수 있습니다.Elasticsearch는 매우 정확한 검색에 탁월한 역량을 발휘합니다\\\\. ESRE를 통해 Elasticsearch는 기업의 독점 데이터에 연결된 생성형 AI를 위한 컨텍스트 창을 제공하여 개발자가 매력적이고 더 정확한 검색 경험을 구축할 수 있도록 합니다\\\\. 검색 결과는 사용자의 원래 쿼리에 따라 반환되며, 개발자는 해당 데이터를 원하는 언어 모델에 전달하여 컨텍스트가 추가된 답변을 제공할 수 있습니다\\\\. Elastic은 비즈니스에 맞춤 설정된 기업의 프라이빗 콘텐츠 저장소의 관련 컨텍스트별 데이터로 질문 답변 및 개인 맞춤 제공 기능을 강화합니다.모든 개발자에게 탁월한 정확도를 바로 제공Elasticsearch Relevance Engine의 릴리즈로 Elastic의 독점 검색 모델을 즉시 사용할 수 있게 되었습니다\\\\. 이 모델은 다운로드하기 쉽고 Elastic 웹 크롤러, 커넥터, API 등 Elastic의 전체 수집 메커니즘 카탈로그와 연동됩니다\\\\. 개발자는 이 모델을 검색 가능한 코퍼스와 함께 즉시 사용할 수 있으며 노트북의 메모리에 들어갈 정도로 작습니다\\\\. Elastic의 Learned Sparse Encoder는 기술 자료, 학술 저널, 법적 발견, 특허 데이터베이스와 같은 검색 사용 사례에 대해 도메인 전반에 걸친 시맨틱 검색을 제공하여 모델 적응이나 훈련 없이 매우 정확한 검색 결과를 제공합니다.\\xa0대부분의 실제 테스트는 하이브리드 순위 기술이 가장 정확한 검색 결과 세트를 생성하고 있음을 보여줍니다', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}),\n",
       " Document(page_content='지금까지는 핵심 구성 요소인 RRF를 놓치고 있었습니다.\\xa0 이제 애플리케이션 검색 요구 사항에 RRF를 포함하고 있으므로 벡터 및 텍스트 검색 기능을 결합할 수 있습니다.\\xa0머신 러닝은 시맨틱 컨텍스트로 검색 결과의 정확도를 높이는 데 앞장서고 있지만, 비용, 복잡성 및 리소스 요구 사항이 너무 많아 개발자가 이를 효과적으로 구현하는 데 어려움을 겪고 있습니다\\\\. 개발자가 매우 정확한 AI 지원 검색을 구축하기 위해서는 전문 머신 러닝 또는 데이터 과학 팀의 지원이 필요한 경우가 일반적입니다\\\\. 이러한 팀은 적절한 모델을 선택하고, 도메인별 데이터 세트로 훈련하고, 데이터 및 해당 관계의 변경으로 인해 모델이 발전함에 따라 모델을 유지 관리하는 데 상당한 시간을 소비합니다.Go1이 확장 가능한 시맨틱 검색을 위해 어떻게 Elastic의 벡터 데이터베이스를 사용하는지 알아보세요.전문 팀의 지원을 받지 못하는 개발자도 시맨틱 검색을 구현할 수 있으며, 다른 대안에 필요한 노력과 전문 지식 없이 처음부터 AI 지원 검색 정확도의 이점을 누릴 수 있습니다\\\\. 오늘부터 모든 고객은 더 뛰어난 정확도와 현대적이고 스마트한 검색을 실현하는 데 도움이 되는 빌딩 블록을 갖추게 됩니다.사용해 보기이러한 기능과 그 외 다양한 기능에 대해 알아보세요.기존 Elastic Cloud 고객은 Elastic Cloud 콘솔에서 이 중 많은 기능을 바로 이용하실 수 있습니다\\\\. 클라우드에서 Elastic을 활용하고 있지 않으신가요? LLM 및 생성형 AI와 함께 Elasticsearch를 사용하는 방법을 확인해 보세요.이 게시물에 설명된 기능의 릴리즈 및 시기는 Elastic의 단독 재량에 따릅니다', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}),\n",
       " Document(page_content='현재 이용할 수 없는 기능은 정시에 또는 전혀 제공되지 않을 수 있습니다.Elastic, Elasticsearch, Elasticsearch Relevance Engine, ESRE, Elastic Learned Sparse Encoder 및 관련 마크는 미국 및 기타 국가에서 Elasticsearch N.V.의 상표, 로고 또는 등록 상표입니다\\\\. 기타 모든 회사 및 제품 이름은 해당 소유자의 상표, 로고 또는 등록 상표입니다.공유하기트위터에서 공유하기링크드인에서 공유하기페이스북에서 공유하기이메일로 공유하기인쇄하기목차Elastic Cloud 무료 체험판 등록하기선택하신 클라우드 서비스 제공자에서 완전히 로드된 배포를 이용하세요\\\\. Elasticsearch 개발사로서 클라우드에 있는 사용자의 Elastic 클러스터에 기능과 지원을 제공해드립니다.무료 체험판 사용 시작팔로우하기회사 소개Elastic 소개우리의 이야기리더십 소개DE&I블로그참여하기커리어커리어 포털언론보도 자료기사파트너파트너 찾기파트너 로그인액세스 요청파트너 되기신뢰 및 보안EthicsPoint 포털보안 및 개인정보보호ECCN 보고서윤리 이메일IR 정보투자자 리소스거버넌스재무주식EXCELLENCE AWARDS이전 수상자ElasticON 투어스폰서가 되세요모든 이벤트회사 소개Elastic 소개우리의 이야기리더십 소개DE&I블로그참여하기커리어커리어 포털언론보도 자료기사파트너파트너 찾기파트너 로그인액세스 요청파트너 되기신뢰 및 보안EthicsPoint 포털보안 및 개인정보보호ECCN 보고서윤리 이메일IR 정보투자자 리소스거버넌스재무주식EXCELLENCE AWARDS이전 수상자ElasticON 투어스폰서가 되세요모든 이벤트상표이용약관개인정보 취급방침사이트맵© \\\\. Elasticsearch B.V', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}),\n",
       " Document(page_content='All rights reserved.Elastic, Elasticsearch 및 기타 관련 상표는 미국 및 기타 국가에서 Elasticsearch B.V.의 상표, 로고 또는 등록 상표입니다.Apache, Apache Lucene, Apache Hadoop, Hadoop, HDFS, 노란색 코끼리 로고는 미국 및/또는 기타 국가에서 Apache Software Foundation의 상표입니다.', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.elastic.co/kr/blog/may-2023-launch-announcement\")\n",
    "loader.requests_kwargs = {'verify':False}\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0, \n",
    "    separator=\"\\. \",\n",
    "    length_function = len,\n",
    "    is_separator_regex = True\n",
    ")\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = vectorstore.from_documents(\n",
    "    docs, \n",
    "    embeddings, \n",
    "    es_connection=client,\n",
    "    index_name= \"workplace_index\",\n",
    "    distance_strategy=\"COSINE\",\n",
    "    strategy=ElasticsearchStore.ApproxRetrievalStrategy(\n",
    "        hybrid=True,\n",
    "        query_model_id=es_model_id\n",
    "    ),\n",
    "    bulk_kwargs={\n",
    "        \"chunk_size\": 500,\n",
    "        \"max_chunk_bytes\": 800000\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showResults(output):\n",
    "  print(\"Total results: \", len(output))\n",
    "  for index in range(len(output)):\n",
    "    print(output[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results:  4\n",
      "page_content='지금까지는 핵심 구성 요소인 RRF를 놓치고 있었습니다.\\xa0 이제 애플리케이션 검색 요구 사항에 RRF를 포함하고 있으므로 벡터 및 텍스트 검색 기능을 결합할 수 있습니다.\\xa0머신 러닝은 시맨틱 컨텍스트로 검색 결과의 정확도를 높이는 데 앞장서고 있지만, 비용, 복잡성 및 리소스 요구 사항이 너무 많아 개발자가 이를 효과적으로 구현하는 데 어려움을 겪고 있습니다\\\\. 개발자가 매우 정확한 AI 지원 검색을 구축하기 위해서는 전문 머신 러닝 또는 데이터 과학 팀의 지원이 필요한 경우가 일반적입니다\\\\. 이러한 팀은 적절한 모델을 선택하고, 도메인별 데이터 세트로 훈련하고, 데이터 및 해당 관계의 변경으로 인해 모델이 발전함에 따라 모델을 유지 관리하는 데 상당한 시간을 소비합니다.Go1이 확장 가능한 시맨틱 검색을 위해 어떻게 Elastic의 벡터 데이터베이스를 사용하는지 알아보세요.전문 팀의 지원을 받지 못하는 개발자도 시맨틱 검색을 구현할 수 있으며, 다른 대안에 필요한 노력과 전문 지식 없이 처음부터 AI 지원 검색 정확도의 이점을 누릴 수 있습니다\\\\. 오늘부터 모든 고객은 더 뛰어난 정확도와 현대적이고 스마트한 검색을 실현하는 데 도움이 되는 빌딩 블록을 갖추게 됩니다.사용해 보기이러한 기능과 그 외 다양한 기능에 대해 알아보세요.기존 Elastic Cloud 고객은 Elastic Cloud 콘솔에서 이 중 많은 기능을 바로 이용하실 수 있습니다\\\\. 클라우드에서 Elastic을 활용하고 있지 않으신가요? LLM 및 생성형 AI와 함께 Elasticsearch를 사용하는 방법을 확인해 보세요.이 게시물에 설명된 기능의 릴리즈 및 시기는 Elastic의 단독 재량에 따릅니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n",
      "page_content='즉, 사용자의 의도를 이해하여 이전에는 볼 수 없었던 구체적인 응답을 제공할 수 있습니다.특히, 모든 검색의 발전은 새로운 기술과 변화하는 사용자 행동으로 인해 제기된 새로운 문제를 해결하는 동시에 더 나은 정확도를 제공합니다\\\\. 시맨틱 검색을 제공하기 위해 키워드 검색을 확장하든 비디오 및 이미지에 대한 새로운 검색 양식을 활성화하든, 새로운 기술에는 검색 사용자에게 더 나은 경험을 제공하기 위한 고유한 도구가 필요합니다\\\\. 마찬가지로 오늘날의 인공 지능 세계에서는 고객이 테스트한 검증된 기능을 갖춘 기술 스택에 구축된 확장성이 뛰어난 새로운 개발자 툴킷이 필요합니다.생성형 AI의 추진력과 더불어 ChatGPT와 같은 기술 채택이 증가하고, 대규모 언어 모델 기능에 대한 인식이 높아지면서 개발자는 애플리케이션을 개선할 수 있는 기술을 실험하고 싶어 합니다\\\\. Elasticsearch Relevance Engine은 생성형 AI 세계에서 새로운 기능의 시대를 열었으며 모든 개발자 팀이 즉시 사용할 수 있는 강력한 도구를 제공합니다.Elasticsearch Relevance Engine은 Elastic Cloud에서 지금 제공되고 있습니다\\\\. 최신 릴리즈의 모든 새로운 기능들은 유일한 호스트형 Elasticsearch 제품인 Elastic Cloud에서만 이용하실 수 있습니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n",
      "page_content='그러나 챗봇과 같은 자체 생성형 AI 앱을 구축하려는 기업은 LLM과 프라이빗 데이터를 결합해야 합니다.Elasticsearch Relevance Engine은 정밀한 컨텍스트 창으로 정확도를 효율적으로 제공하는 엔진을 기업에 제공하여 번거로움이나 비용 없이 데이터 공간을 줄일 수 있도록 돕습니다.오래된 데이터: 모델은 데이터가 수집되는 시점에 고정됩니다\\\\. 따라서 생성형 AI 모델이 생성하는 콘텐츠와 데이터는 훈련에 사용된 데이터만큼만 최신입니다\\\\. LLM에서 결과를 적시에 도출하기 위해서는 기업 데이터 통합이 본질적으로 필요합니다.환각: 질문에 답하거나 모델과 대화할 때 모델은 신뢰할 수 있고 설득력 있게 들리는 사실을 만들어 낼 수 있지만 실제로는 사실이 아닌 예측입니다\\\\. 이것이 비즈니스 컨텍스트에서 유용한 모델을 만들려면 LLM을 상황에 맞는 맞춤형 지식으로 교육하는 것이 매우 중요한 또 다른 이유입니다.Elasticsearch Relevance Engine을 사용하면 개발자가 생성형 AI 모델의 컨텍스트 창을 통해 자체 데이터 저장소에 연결할 수 있습니다\\\\. 추가된 검색 결과는 프라이빗 소스 또는 전문 도메인의 최신 정보를 제공할 수 있으므로 질문을 받았을 때 모델의 소위 ‘파라메트릭’ 지식에만 의존할 때 보다 더 사실에 기반한 정보를 반환할 수 있습니다.벡터 데이터베이스로 강화Elasticsearch Relevance Engine에는 복원력을 갖춘 프로덕션 등급 벡터 데이터베이스가 내재화되어 있습니다\\\\. 이는 개발자에게 풍부한 시맨틱 검색 애플리케이션을 구축할 수 있는 기반을 제공합니다\\\\. 개발 팀은 Elastic의 플랫폼에서 밀집 벡터 검색을 사용하여 키워드나 동의어로 제한되지 않는 보다 직관적인 질문 답변을 생성할 수 있습니다\\\\. 이미지와 같은 비정형 데이터를 사용하여 멀티 모달 검색을 구축하고, 심지어 사용자 프로필을 모델링하고 일치 항목을 생성하여 제품 및 검색, 구직 또는 중매 애플리케이션에서 검색 결과를 개인에 맞게 제공할 수 있습니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n",
      "page_content='ESRE는 검색 분야를 선도하는 Elastic의 리더십과 2년에 걸친 머신 러닝 연구 및 개발을 기반으로 구축되었습니다\\\\. Elasticsearch Relevance Engine은 AI의 장점과 Elastic의 텍스트 검색을 결합합니다\\\\. ESRE는 개발자들에게 정교한 검색 알고리즘의 전체 제품군과 대규모 언어 모델(LLM)과 통합할 수 있는 기능을 제공합니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n"
     ]
    }
   ],
   "source": [
    "query = \"시맨틱 검색이란?\"\n",
    "results = db.similarity_search(query)\n",
    "\n",
    "showResults(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results:  9\n",
      "page_content='All rights reserved.Elastic, Elasticsearch 및 기타 관련 상표는 미국 및 기타 국가에서 Elasticsearch B.V.의 상표, 로고 또는 등록 상표입니다.Apache, Apache Lucene, Apache Hadoop, Hadoop, HDFS, 노란색 코끼리 로고는 미국 및/또는 기타 국가에서 Apache Software Foundation의 상표입니다.' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n",
      "page_content='ESRE는 검색 분야를 선도하는 Elastic의 리더십과 2년에 걸친 머신 러닝 연구 및 개발을 기반으로 구축되었습니다\\\\. Elasticsearch Relevance Engine은 AI의 장점과 Elastic의 텍스트 검색을 결합합니다\\\\. ESRE는 개발자들에게 정교한 검색 알고리즘의 전체 제품군과 대규모 언어 모델(LLM)과 통합할 수 있는 기능을 제공합니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n",
      "page_content='그러나 챗봇과 같은 자체 생성형 AI 앱을 구축하려는 기업은 LLM과 프라이빗 데이터를 결합해야 합니다.Elasticsearch Relevance Engine은 정밀한 컨텍스트 창으로 정확도를 효율적으로 제공하는 엔진을 기업에 제공하여 번거로움이나 비용 없이 데이터 공간을 줄일 수 있도록 돕습니다.오래된 데이터: 모델은 데이터가 수집되는 시점에 고정됩니다\\\\. 따라서 생성형 AI 모델이 생성하는 콘텐츠와 데이터는 훈련에 사용된 데이터만큼만 최신입니다\\\\. LLM에서 결과를 적시에 도출하기 위해서는 기업 데이터 통합이 본질적으로 필요합니다.환각: 질문에 답하거나 모델과 대화할 때 모델은 신뢰할 수 있고 설득력 있게 들리는 사실을 만들어 낼 수 있지만 실제로는 사실이 아닌 예측입니다\\\\. 이것이 비즈니스 컨텍스트에서 유용한 모델을 만들려면 LLM을 상황에 맞는 맞춤형 지식으로 교육하는 것이 매우 중요한 또 다른 이유입니다.Elasticsearch Relevance Engine을 사용하면 개발자가 생성형 AI 모델의 컨텍스트 창을 통해 자체 데이터 저장소에 연결할 수 있습니다\\\\. 추가된 검색 결과는 프라이빗 소스 또는 전문 도메인의 최신 정보를 제공할 수 있으므로 질문을 받았을 때 모델의 소위 ‘파라메트릭’ 지식에만 의존할 때 보다 더 사실에 기반한 정보를 반환할 수 있습니다.벡터 데이터베이스로 강화Elasticsearch Relevance Engine에는 복원력을 갖춘 프로덕션 등급 벡터 데이터베이스가 내재화되어 있습니다\\\\. 이는 개발자에게 풍부한 시맨틱 검색 애플리케이션을 구축할 수 있는 기반을 제공합니다\\\\. 개발 팀은 Elastic의 플랫폼에서 밀집 벡터 검색을 사용하여 키워드나 동의어로 제한되지 않는 보다 직관적인 질문 답변을 생성할 수 있습니다\\\\. 이미지와 같은 비정형 데이터를 사용하여 멀티 모달 검색을 구축하고, 심지어 사용자 프로필을 모델링하고 일치 항목을 생성하여 제품 및 검색, 구직 또는 중매 애플리케이션에서 검색 결과를 개인에 맞게 제공할 수 있습니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n",
      "page_content='또한 Elastic Stack과 클라우드 오케스트레이션 제품들, Elastic Cloud Enterprise와 Kubernetes의 Elastic Cloud를 다운로드하셔서 자체 관리형 환경을 만드셔도 됩니다.Elasticsearch Relevance Engine™에 대해 자세히 알고 싶으신가요? 다음 기술 블로그를 확인하세요.ChatGPT와 ElasticsearchElastic Learned Sparse Encoder 블로그Elastic에서 머신 러닝 모델에 액세스LangChain 및 Elasticsearch를 사용한 개인정보 보호 우선 AI 검색생성형 AI 모델의 한계 극복Elasticsearch Relevance Engine™은 개발자가 빠르게 발전하고 생성형 AI를 비롯하여 자연어 검색의 이러한 문제를 해결할 수 있도록 지원합니다.엔터프라이즈 데이터/컨텍스트 인식: 모델에 특정 도메인과 관련된 충분한 내부 지식이 없을 수 있습니다\\\\. 이는 모델 훈련에 사용된 데이터 세트에 따라 달라집니다\\\\. 기업이 LLM에서 생성하는 데이터와 콘텐츠를 맞춤화하기 위해서는 모델이 더 정확하고 비즈니스에 특화된 정보를 배울 수 있도록 모델에 독점 데이터를 제공하는 방법이 필요합니다.탁월한 정확도: Elasticsearch Relevance Engine은 시맨틱 검색을 사용하여 컨텍스트를 검색하기 위해 벡터 임베딩을 생성하고 저장하는 것처럼 간단하게 프라이빗 소스의 데이터를 통합합니다\\\\. 벡터 임베딩은 단어, 구문 또는 문서의 숫자 표현으로, LLM이 단어의 의미와 해당 관계를 이해하는 데 도움이 됩니다\\\\. 이러한 임베딩은 속도와 확장성 측면에서 트랜스포머 모델의 출력을 향상시킵니다\\\\. ESRE는 또한 개발자가 자체 트랜스포머 모델을 Elastic으로 가져오거나 서드파티 모델과 통합할 수 있도록 합니다.Elastic은 또한 최신 상호 작용 모델의 출현으로 서드파티 데이터 세트에 대한 광범위한 훈련이나 미세 조정 없이도 이를 즉시 제공할 수 있다는 것을 깨달았습니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n",
      "page_content='이러한 NLP 트랜스포머 모델은 또한 정서 분석, 명명된 엔터티 인식, 텍스트 분류와 같은 머신 러닝 작업을 지원합니다\\\\. Elastic의 벡터 데이터베이스를 사용하면 개발자가 실제 프로덕션 애플리케이션을 위해 확장성과 성능이 뛰어난 임베딩을 생성, 저장 및 쿼리할 수 있습니다.Elasticsearch는 매우 정확한 검색에 탁월한 역량을 발휘합니다\\\\. ESRE를 통해 Elasticsearch는 기업의 독점 데이터에 연결된 생성형 AI를 위한 컨텍스트 창을 제공하여 개발자가 매력적이고 더 정확한 검색 경험을 구축할 수 있도록 합니다\\\\. 검색 결과는 사용자의 원래 쿼리에 따라 반환되며, 개발자는 해당 데이터를 원하는 언어 모델에 전달하여 컨텍스트가 추가된 답변을 제공할 수 있습니다\\\\. Elastic은 비즈니스에 맞춤 설정된 기업의 프라이빗 콘텐츠 저장소의 관련 컨텍스트별 데이터로 질문 답변 및 개인 맞춤 제공 기능을 강화합니다.모든 개발자에게 탁월한 정확도를 바로 제공Elasticsearch Relevance Engine의 릴리즈로 Elastic의 독점 검색 모델을 즉시 사용할 수 있게 되었습니다\\\\. 이 모델은 다운로드하기 쉽고 Elastic 웹 크롤러, 커넥터, API 등 Elastic의 전체 수집 메커니즘 카탈로그와 연동됩니다\\\\. 개발자는 이 모델을 검색 가능한 코퍼스와 함께 즉시 사용할 수 있으며 노트북의 메모리에 들어갈 정도로 작습니다\\\\. Elastic의 Learned Sparse Encoder는 기술 자료, 학술 저널, 법적 발견, 특허 데이터베이스와 같은 검색 사용 사례에 대해 도메인 전반에 걸친 시맨틱 검색을 제공하여 모델 적응이나 훈련 없이 매우 정확한 검색 결과를 제공합니다.\\xa0대부분의 실제 테스트는 하이브리드 순위 기술이 가장 정확한 검색 결과 세트를 생성하고 있음을 보여줍니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n",
      "page_content='지금까지는 핵심 구성 요소인 RRF를 놓치고 있었습니다.\\xa0 이제 애플리케이션 검색 요구 사항에 RRF를 포함하고 있으므로 벡터 및 텍스트 검색 기능을 결합할 수 있습니다.\\xa0머신 러닝은 시맨틱 컨텍스트로 검색 결과의 정확도를 높이는 데 앞장서고 있지만, 비용, 복잡성 및 리소스 요구 사항이 너무 많아 개발자가 이를 효과적으로 구현하는 데 어려움을 겪고 있습니다\\\\. 개발자가 매우 정확한 AI 지원 검색을 구축하기 위해서는 전문 머신 러닝 또는 데이터 과학 팀의 지원이 필요한 경우가 일반적입니다\\\\. 이러한 팀은 적절한 모델을 선택하고, 도메인별 데이터 세트로 훈련하고, 데이터 및 해당 관계의 변경으로 인해 모델이 발전함에 따라 모델을 유지 관리하는 데 상당한 시간을 소비합니다.Go1이 확장 가능한 시맨틱 검색을 위해 어떻게 Elastic의 벡터 데이터베이스를 사용하는지 알아보세요.전문 팀의 지원을 받지 못하는 개발자도 시맨틱 검색을 구현할 수 있으며, 다른 대안에 필요한 노력과 전문 지식 없이 처음부터 AI 지원 검색 정확도의 이점을 누릴 수 있습니다\\\\. 오늘부터 모든 고객은 더 뛰어난 정확도와 현대적이고 스마트한 검색을 실현하는 데 도움이 되는 빌딩 블록을 갖추게 됩니다.사용해 보기이러한 기능과 그 외 다양한 기능에 대해 알아보세요.기존 Elastic Cloud 고객은 Elastic Cloud 콘솔에서 이 중 많은 기능을 바로 이용하실 수 있습니다\\\\. 클라우드에서 Elastic을 활용하고 있지 않으신가요? LLM 및 생성형 AI와 함께 Elasticsearch를 사용하는 방법을 확인해 보세요.이 게시물에 설명된 기능의 릴리즈 및 시기는 Elastic의 단독 재량에 따릅니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n",
      "page_content='AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic BlogSkip to main contentPlatformPlatform솔루션솔루션고객고객리소스리소스요금제요금제설명서설명서Language pickerDeutschEnglishEspañolFrançais日本語한국어简体中文PortuguêsSearchLogin무료 체험판 사용 시작영업팀에 문의하기Elasticsearch PlatformElasticsearch Platform이 제공하는 Observability, Security, Search 솔루션Elastic 개요ELK Stack검색, 분석, 데이터 수집, 시각화를 간편하게KibanaElasticsearch통합ELK Stack 개요Elastic Cloud원하는 클라우드 서비스 제공자에서 Elastic을 사용하여 중요한 답을 찾아보세요.Cloud 개요파트너영업팀에 문의하기Observability앱과 인프라 가시성을 통합하여 문제를 사전에 해결합니다.로그 모니터링애플리케이션 성능 모니터링인프라 모니터링가상 모니터링실제 사용자 모니터링Universal ProfilingAIOpsOpenTelemetryObservability 개요Security대규모 사이버 위협으로부터 신속하게 보호하고 조사하고 대응하세요.지속적 모니터링위협 헌팅조사 및 사고 대응자동화된 위협 보호Security 개요Search모든 클라우드에 걸쳐 검색 결과를 가속화하고 맞춤형 검색을 강화하세요.생성형 AI검색 애플리케이션전자 상거래웹사이트Workplace Search고객 지원Search 개요산업별공공 부문금융 서비스통신의료기술소매 및 전자 상거래제조 및 자동차모든 산업 보기솔루션별고객에게 미래에 대비할 수 있는 유연성, 속도, 규모를 제공합니다.ObservabilitySecuritySearch고객 스포트라이트Cisco가 어떻게 AI를 통해 검색 경험을 혁신하는지 알아보세요.자세히 알아보기RWE가 어떻게 재생 에너지 거래 사업을 지원하는지 알아보세요.자세히 알아보기Comcast가 어떻게 엔지니어링 속도를 높이고 혁신을 강화하는지 알아보세요.자세히 알아보기개발자코드, 포럼, 그룹과 관련된 모든 것을 알아보세요.커뮤니티포럼다운로드소통최신 기술 주제, 혁신, 뉴스로 최신 정보를 받아보세요.이벤트블로그학습역량을 강화하고 성공을 위한 기회를 마련하세요.시작하기Elastic 리소스컨설팅 서비스교육 및 인증Security Labs도움말어떤 주제이든 필요한 지원을 찾아보세요.문의하기지원 센터Elastic에서 어떤 일이 진행되고 있는지 확인데모 갤러리 보기자세히 알아보기Elasticsearch로 시작하세요지금 시청하기Elastic 8.9의 새로운 기능자세히 알아보기SearchPlatformElasticsearch PlatformElasticsearch Platform이 제공하는 Observability, Security, Search 솔루션Elastic 개요ELK Stack검색, 분석, 데이터 수집, 시각화를 간편하게KibanaElasticsearch통합ELK Stack 개요Elastic Cloud원하는 클라우드 서비스 제공자에서 Elastic을 사용하여 중요한 답을 찾아보세요.Cloud 개요파트너영업팀에 문의하기솔루션Observability앱과 인프라 가시성을 통합하여 문제를 사전에 해결합니다.로그 모니터링애플리케이션 성능 모니터링인프라 모니터링가상 모니터링실제 사용자 모니터링Universal ProfilingAIOpsOpenTelemetryObservability 개요Security대규모 사이버 위협으로부터 신속하게 보호하고 조사하고 대응하세요.지속적 모니터링위협 헌팅조사 및 사고 대응자동화된 위협 보호Security 개요Search모든 클라우드에 걸쳐 검색 결과를 가속화하고 맞춤형 검색을 강화하세요.생성형 AI검색 애플리케이션전자 상거래웹사이트Workplace Search고객 지원Search 개요산업별공공 부문금융 서비스통신의료기술소매 및 전자 상거래제조 및 자동차모든 산업 보기고객솔루션별고객에게 미래에 대비할 수 있는 유연성, 속도, 규모를 제공합니다.ObservabilitySecuritySearch고객 스포트라이트Cisco가 어떻게 AI를 통해 검색 경험을 혁신하는지 알아보세요.자세히 알아보기RWE가 어떻게 재생 에너지 거래 사업을 지원하는지 알아보세요.자세히 알아보기Comcast가 어떻게 엔지니어링 속도를 높이고 혁신을 강화하는지 알아보세요.자세히 알아보기리소스개발자코드, 포럼, 그룹과 관련된 모든 것을 알아보세요.커뮤니티포럼다운로드소통최신 기술 주제, 혁신, 뉴스로 최신 정보를 받아보세요.이벤트블로그학습역량을 강화하고 성공을 위한 기회를 마련하세요.시작하기Elastic 리소스컨설팅 서비스교육 및 인증Security Labs도움말어떤 주제이든 필요한 지원을 찾아보세요.문의하기지원 센터Elastic에서 어떤 일이 진행되고 있는지 확인데모 갤러리 보기자세히 알아보기Elasticsearch로 시작하세요지금 시청하기Elastic 8.9의 새로운 기능자세히 알아보기요금제설명서Language picker 언어DeutschEnglishEspañolFrançais日本語한국어简体中文PortuguêsLogin 로그인무료 체험판 사용 시작영업팀에 문의하기블로그솔루션Stack + Cloud기술 주제소식고객인사이트AI 혁신을 위한 고급 검색, Elasticsearch Relevance Engine™ 소개ByMatt Riley2023년 5월 23일목차트위터에서 공유하기링크드인에서 공유하기페이스북에서 공유하기이메일로 공유하기인쇄하기오늘은 매우 정확한 AI 검색 애플리케이션을 만들기 위한 새로운 기능인 Elasticsearch Relevance Engine™(ESRE™)을 소개하겠습니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n",
      "page_content='더 좋은 점은 Elastic 커뮤니티에서 이미 신뢰하고 있는 간단한 통합 API를 통해 액세스할 수 있기 때문에 전 세계 개발자들이 즉시 이를 사용하여 검색 정확도를 높일 수 있다는 것입니다.Elasticsearch Relevance Engine의 구성 가능한 기능은 다음과 같은 방법으로 정확도를 개선하는 데 도움이 될 수 있습니다.하이브리드 검색의 중요한 구성 요소인 BM25f 등 고급 정확도 순위 지정 기능 적용Elastic의 벡터 데이터베이스를 사용하여 밀집 임베딩 생성, 저장 및 검색다양한 자연어 처리(NLP) 작업 및 모델을 사용하여 텍스트 처리개발자가 비즈니스 관련 컨텍스트에 맞게 Elastic에서 자체 트랜스포머 모델을 관리 및 사용하도록 허용API를 통해 OpenAI의 GPT-3 및 4와 같은 서드파티 트랜스포머 모델과 통합하여 Elasticsearch 배포 내에 통합된 고객의 데이터 저장소를 기반으로 콘텐츠의 직관적인 요약을 검색Elastic의 즉시 사용 가능한 Learned Sparse Encoder 모델을 사용하여 모델을 훈련하거나 유지관리하지 않고 ML 지원 검색을 활성화하여 다양한 도메인에 걸쳐 매우 정확한 시맨틱 검색 제공개발자가 AI 검색 엔진을 자연어 및 키워드 쿼리 유형의 고유한 조합에 최적화하도록 제어할 수 있는 하이브리드 순위 지정 방법인 상호 순위 결합(RRF)을 사용하여 희소 검색과 밀접 검색을 손쉽게 결합LangChain과 같은 서드파티 도구와 통합하여 정교한 데이터 파이프라인 및 생성형 AI 애플리케이션 구축 지원검색의 진화는 정확도를 개선하고 검색 애플리케이션과 상호 작용하는 방식을 개선해야 하는 지속적인 필요성에 의해 주도되어 왔습니다\\\\. 매우 정확한 검색 결과는 검색 앱에 대한 사용자 참여를 높이고 수익과 생산성 모두에 상당한 다운스트림 영향을 미칠 수 있습니다\\\\. LLM 및 생성형 AI의 새로운 세계에서 검색은 한 단계 더 나아갈 수 있습니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n",
      "page_content='모든 개발 팀이 머신 러닝 모델을 훈련 및 유지관리하거나 확장성, 성능 및 속도에 대한 절충을 이해할 수 있는 리소스나 전문 지식을 보유하고 있는 것은 아니기 때문에 Elasticsearch Relevance Engine에는 다양한 도메인에 걸친 시맨틱 검색을 위해 구축된 검색 모델인 Elastic Learned Sparse Encoder도 포함되어 있습니다\\\\. 이 모델은 희소 임베딩을 기존의 키워드 기반 BM25 검색과 결합하여 하이브리드 검색을 위해 사용하기 쉬운 상호 순위 결합(RRF) 점수를 확보합니다\\\\. 개발자들은 ESRE를 통해 사용 첫날부터 머신 러닝 지원 정확도 및 하이브리드 검색 기술을 활용할 수 있습니다.개인정보 보호 및 보안: 데이터 개인정보 보호는 기업이 네트워크를 통해 그리고 구성 요소 간에 독점 데이터를 사용하고 안전하게 전달하는 방법에서 핵심적인 역할을 합니다\\\\. 이는 혁신적인 검색 경험을 구축하는 경우에도 마찬가지입니다.Elastic은 역할 기반 및 속성 기반 액세스 제어를 기본적으로 지원하여 채팅 및 질문 답변 애플리케이션에서도 데이터에 대한 액세스 권한이 있는 역할만 데이터를 볼 수 있도록 합니다\\\\. Elasticsearch는 권한이 있는 개인만 특정 문서에 액세스할 수 있도록 유지해야 하는 조직의 요구를 지원함으로써 모든 검색 애플리케이션에서 보편적인 개인정보 보호 및 액세스 제어를 유지할 수 있도록 돕습니다.개인정보 보호가 최대 관심사인 경우, 모든 데이터를 조직의 네트워크 내에 유지하는 것이 매우 중요할 뿐만 아니라 의무 사항이 될 수 있습니다\\\\. ESRE는 조직이 에어 갭 환경에서 배포를 구현할 수 있도록 허용하는 것부터 보안 네트워크에 대한 액세스를 지원하는 것까지 조직이 데이터를 안전하게 유지하는 데 필요한 도구를 제공합니다.규모와 비용: 데이터 볼륨과 필요한 컴퓨팅 성능 및 메모리는 많은 기업에서 대규모 언어 모델을 사용하지 못하는 장벽이 될 수 있습니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n"
     ]
    }
   ],
   "source": [
    "query = \"How does the compensation work?\"\n",
    "results = db.similarity_search(query, k=10)\n",
    "\n",
    "showResults(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results:  4\n",
      "page_content='즉, 사용자의 의도를 이해하여 이전에는 볼 수 없었던 구체적인 응답을 제공할 수 있습니다.특히, 모든 검색의 발전은 새로운 기술과 변화하는 사용자 행동으로 인해 제기된 새로운 문제를 해결하는 동시에 더 나은 정확도를 제공합니다\\\\. 시맨틱 검색을 제공하기 위해 키워드 검색을 확장하든 비디오 및 이미지에 대한 새로운 검색 양식을 활성화하든, 새로운 기술에는 검색 사용자에게 더 나은 경험을 제공하기 위한 고유한 도구가 필요합니다\\\\. 마찬가지로 오늘날의 인공 지능 세계에서는 고객이 테스트한 검증된 기능을 갖춘 기술 스택에 구축된 확장성이 뛰어난 새로운 개발자 툴킷이 필요합니다.생성형 AI의 추진력과 더불어 ChatGPT와 같은 기술 채택이 증가하고, 대규모 언어 모델 기능에 대한 인식이 높아지면서 개발자는 애플리케이션을 개선할 수 있는 기술을 실험하고 싶어 합니다\\\\. Elasticsearch Relevance Engine은 생성형 AI 세계에서 새로운 기능의 시대를 열었으며 모든 개발자 팀이 즉시 사용할 수 있는 강력한 도구를 제공합니다.Elasticsearch Relevance Engine은 Elastic Cloud에서 지금 제공되고 있습니다\\\\. 최신 릴리즈의 모든 새로운 기능들은 유일한 호스트형 Elasticsearch 제품인 Elastic Cloud에서만 이용하실 수 있습니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n",
      "page_content='그러나 챗봇과 같은 자체 생성형 AI 앱을 구축하려는 기업은 LLM과 프라이빗 데이터를 결합해야 합니다.Elasticsearch Relevance Engine은 정밀한 컨텍스트 창으로 정확도를 효율적으로 제공하는 엔진을 기업에 제공하여 번거로움이나 비용 없이 데이터 공간을 줄일 수 있도록 돕습니다.오래된 데이터: 모델은 데이터가 수집되는 시점에 고정됩니다\\\\. 따라서 생성형 AI 모델이 생성하는 콘텐츠와 데이터는 훈련에 사용된 데이터만큼만 최신입니다\\\\. LLM에서 결과를 적시에 도출하기 위해서는 기업 데이터 통합이 본질적으로 필요합니다.환각: 질문에 답하거나 모델과 대화할 때 모델은 신뢰할 수 있고 설득력 있게 들리는 사실을 만들어 낼 수 있지만 실제로는 사실이 아닌 예측입니다\\\\. 이것이 비즈니스 컨텍스트에서 유용한 모델을 만들려면 LLM을 상황에 맞는 맞춤형 지식으로 교육하는 것이 매우 중요한 또 다른 이유입니다.Elasticsearch Relevance Engine을 사용하면 개발자가 생성형 AI 모델의 컨텍스트 창을 통해 자체 데이터 저장소에 연결할 수 있습니다\\\\. 추가된 검색 결과는 프라이빗 소스 또는 전문 도메인의 최신 정보를 제공할 수 있으므로 질문을 받았을 때 모델의 소위 ‘파라메트릭’ 지식에만 의존할 때 보다 더 사실에 기반한 정보를 반환할 수 있습니다.벡터 데이터베이스로 강화Elasticsearch Relevance Engine에는 복원력을 갖춘 프로덕션 등급 벡터 데이터베이스가 내재화되어 있습니다\\\\. 이는 개발자에게 풍부한 시맨틱 검색 애플리케이션을 구축할 수 있는 기반을 제공합니다\\\\. 개발 팀은 Elastic의 플랫폼에서 밀집 벡터 검색을 사용하여 키워드나 동의어로 제한되지 않는 보다 직관적인 질문 답변을 생성할 수 있습니다\\\\. 이미지와 같은 비정형 데이터를 사용하여 멀티 모달 검색을 구축하고, 심지어 사용자 프로필을 모델링하고 일치 항목을 생성하여 제품 및 검색, 구직 또는 중매 애플리케이션에서 검색 결과를 개인에 맞게 제공할 수 있습니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n",
      "page_content='지금까지는 핵심 구성 요소인 RRF를 놓치고 있었습니다.\\xa0 이제 애플리케이션 검색 요구 사항에 RRF를 포함하고 있으므로 벡터 및 텍스트 검색 기능을 결합할 수 있습니다.\\xa0머신 러닝은 시맨틱 컨텍스트로 검색 결과의 정확도를 높이는 데 앞장서고 있지만, 비용, 복잡성 및 리소스 요구 사항이 너무 많아 개발자가 이를 효과적으로 구현하는 데 어려움을 겪고 있습니다\\\\. 개발자가 매우 정확한 AI 지원 검색을 구축하기 위해서는 전문 머신 러닝 또는 데이터 과학 팀의 지원이 필요한 경우가 일반적입니다\\\\. 이러한 팀은 적절한 모델을 선택하고, 도메인별 데이터 세트로 훈련하고, 데이터 및 해당 관계의 변경으로 인해 모델이 발전함에 따라 모델을 유지 관리하는 데 상당한 시간을 소비합니다.Go1이 확장 가능한 시맨틱 검색을 위해 어떻게 Elastic의 벡터 데이터베이스를 사용하는지 알아보세요.전문 팀의 지원을 받지 못하는 개발자도 시맨틱 검색을 구현할 수 있으며, 다른 대안에 필요한 노력과 전문 지식 없이 처음부터 AI 지원 검색 정확도의 이점을 누릴 수 있습니다\\\\. 오늘부터 모든 고객은 더 뛰어난 정확도와 현대적이고 스마트한 검색을 실현하는 데 도움이 되는 빌딩 블록을 갖추게 됩니다.사용해 보기이러한 기능과 그 외 다양한 기능에 대해 알아보세요.기존 Elastic Cloud 고객은 Elastic Cloud 콘솔에서 이 중 많은 기능을 바로 이용하실 수 있습니다\\\\. 클라우드에서 Elastic을 활용하고 있지 않으신가요? LLM 및 생성형 AI와 함께 Elasticsearch를 사용하는 방법을 확인해 보세요.이 게시물에 설명된 기능의 릴리즈 및 시기는 Elastic의 단독 재량에 따릅니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n",
      "page_content='ESRE는 검색 분야를 선도하는 Elastic의 리더십과 2년에 걸친 머신 러닝 연구 및 개발을 기반으로 구축되었습니다\\\\. Elasticsearch Relevance Engine은 AI의 장점과 Elastic의 텍스트 검색을 결합합니다\\\\. ESRE는 개발자들에게 정교한 검색 알고리즘의 전체 제품군과 대규모 언어 모델(LLM)과 통합할 수 있는 기능을 제공합니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n"
     ]
    }
   ],
   "source": [
    "query = \"생성형 AI가 생성하는 콘텐츠는 무엇일까?\"\n",
    "results = db.similarity_search(\n",
    "    query\n",
    ")\n",
    "\n",
    "showResults(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results:  4\n",
      "page_content='ESRE는 검색 분야를 선도하는 Elastic의 리더십과 2년에 걸친 머신 러닝 연구 및 개발을 기반으로 구축되었습니다\\\\. Elasticsearch Relevance Engine은 AI의 장점과 Elastic의 텍스트 검색을 결합합니다\\\\. ESRE는 개발자들에게 정교한 검색 알고리즘의 전체 제품군과 대규모 언어 모델(LLM)과 통합할 수 있는 기능을 제공합니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n",
      "page_content='모든 개발 팀이 머신 러닝 모델을 훈련 및 유지관리하거나 확장성, 성능 및 속도에 대한 절충을 이해할 수 있는 리소스나 전문 지식을 보유하고 있는 것은 아니기 때문에 Elasticsearch Relevance Engine에는 다양한 도메인에 걸친 시맨틱 검색을 위해 구축된 검색 모델인 Elastic Learned Sparse Encoder도 포함되어 있습니다\\\\. 이 모델은 희소 임베딩을 기존의 키워드 기반 BM25 검색과 결합하여 하이브리드 검색을 위해 사용하기 쉬운 상호 순위 결합(RRF) 점수를 확보합니다\\\\. 개발자들은 ESRE를 통해 사용 첫날부터 머신 러닝 지원 정확도 및 하이브리드 검색 기술을 활용할 수 있습니다.개인정보 보호 및 보안: 데이터 개인정보 보호는 기업이 네트워크를 통해 그리고 구성 요소 간에 독점 데이터를 사용하고 안전하게 전달하는 방법에서 핵심적인 역할을 합니다\\\\. 이는 혁신적인 검색 경험을 구축하는 경우에도 마찬가지입니다.Elastic은 역할 기반 및 속성 기반 액세스 제어를 기본적으로 지원하여 채팅 및 질문 답변 애플리케이션에서도 데이터에 대한 액세스 권한이 있는 역할만 데이터를 볼 수 있도록 합니다\\\\. Elasticsearch는 권한이 있는 개인만 특정 문서에 액세스할 수 있도록 유지해야 하는 조직의 요구를 지원함으로써 모든 검색 애플리케이션에서 보편적인 개인정보 보호 및 액세스 제어를 유지할 수 있도록 돕습니다.개인정보 보호가 최대 관심사인 경우, 모든 데이터를 조직의 네트워크 내에 유지하는 것이 매우 중요할 뿐만 아니라 의무 사항이 될 수 있습니다\\\\. ESRE는 조직이 에어 갭 환경에서 배포를 구현할 수 있도록 허용하는 것부터 보안 네트워크에 대한 액세스를 지원하는 것까지 조직이 데이터를 안전하게 유지하는 데 필요한 도구를 제공합니다.규모와 비용: 데이터 볼륨과 필요한 컴퓨팅 성능 및 메모리는 많은 기업에서 대규모 언어 모델을 사용하지 못하는 장벽이 될 수 있습니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n",
      "page_content='즉, 사용자의 의도를 이해하여 이전에는 볼 수 없었던 구체적인 응답을 제공할 수 있습니다.특히, 모든 검색의 발전은 새로운 기술과 변화하는 사용자 행동으로 인해 제기된 새로운 문제를 해결하는 동시에 더 나은 정확도를 제공합니다\\\\. 시맨틱 검색을 제공하기 위해 키워드 검색을 확장하든 비디오 및 이미지에 대한 새로운 검색 양식을 활성화하든, 새로운 기술에는 검색 사용자에게 더 나은 경험을 제공하기 위한 고유한 도구가 필요합니다\\\\. 마찬가지로 오늘날의 인공 지능 세계에서는 고객이 테스트한 검증된 기능을 갖춘 기술 스택에 구축된 확장성이 뛰어난 새로운 개발자 툴킷이 필요합니다.생성형 AI의 추진력과 더불어 ChatGPT와 같은 기술 채택이 증가하고, 대규모 언어 모델 기능에 대한 인식이 높아지면서 개발자는 애플리케이션을 개선할 수 있는 기술을 실험하고 싶어 합니다\\\\. Elasticsearch Relevance Engine은 생성형 AI 세계에서 새로운 기능의 시대를 열었으며 모든 개발자 팀이 즉시 사용할 수 있는 강력한 도구를 제공합니다.Elasticsearch Relevance Engine은 Elastic Cloud에서 지금 제공되고 있습니다\\\\. 최신 릴리즈의 모든 새로운 기능들은 유일한 호스트형 Elasticsearch 제품인 Elastic Cloud에서만 이용하실 수 있습니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n",
      "page_content='지금까지는 핵심 구성 요소인 RRF를 놓치고 있었습니다.\\xa0 이제 애플리케이션 검색 요구 사항에 RRF를 포함하고 있으므로 벡터 및 텍스트 검색 기능을 결합할 수 있습니다.\\xa0머신 러닝은 시맨틱 컨텍스트로 검색 결과의 정확도를 높이는 데 앞장서고 있지만, 비용, 복잡성 및 리소스 요구 사항이 너무 많아 개발자가 이를 효과적으로 구현하는 데 어려움을 겪고 있습니다\\\\. 개발자가 매우 정확한 AI 지원 검색을 구축하기 위해서는 전문 머신 러닝 또는 데이터 과학 팀의 지원이 필요한 경우가 일반적입니다\\\\. 이러한 팀은 적절한 모델을 선택하고, 도메인별 데이터 세트로 훈련하고, 데이터 및 해당 관계의 변경으로 인해 모델이 발전함에 따라 모델을 유지 관리하는 데 상당한 시간을 소비합니다.Go1이 확장 가능한 시맨틱 검색을 위해 어떻게 Elastic의 벡터 데이터베이스를 사용하는지 알아보세요.전문 팀의 지원을 받지 못하는 개발자도 시맨틱 검색을 구현할 수 있으며, 다른 대안에 필요한 노력과 전문 지식 없이 처음부터 AI 지원 검색 정확도의 이점을 누릴 수 있습니다\\\\. 오늘부터 모든 고객은 더 뛰어난 정확도와 현대적이고 스마트한 검색을 실현하는 데 도움이 되는 빌딩 블록을 갖추게 됩니다.사용해 보기이러한 기능과 그 외 다양한 기능에 대해 알아보세요.기존 Elastic Cloud 고객은 Elastic Cloud 콘솔에서 이 중 많은 기능을 바로 이용하실 수 있습니다\\\\. 클라우드에서 Elastic을 활용하고 있지 않으신가요? LLM 및 생성형 AI와 함께 Elasticsearch를 사용하는 방법을 확인해 보세요.이 게시물에 설명된 기능의 릴리즈 및 시기는 Elastic의 단독 재량에 따릅니다' metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}\n"
     ]
    }
   ],
   "source": [
    "query = \"LLM의 역할을 알려줘?\"\n",
    "results = db.similarity_search(\n",
    "    query\n",
    ")\n",
    "\n",
    "showResults(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (0.2.11)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from llama-cpp-python) (4.8.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from llama-cpp-python) (1.23.5)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\programdata\\miniconda3\\envs\\chatbot\\lib\\site-packages (from llama-cpp-python) (5.6.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 0  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path = cwd + \"/models/Llama-2-ko-7B-chat-gguf-q8_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "\n",
    "    # https://www.reddit.com/r/LocalLLaMA/comments/1343bgz/what_model_parameters_is_everyone_using/\n",
    "    temperature=0.7,\n",
    "    top_k=40,\n",
    "    top_p=0.1,\n",
    "\n",
    "    max_tokens=1024,\n",
    "    verbose=True,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요.Large Language Model은 언어 모델의 크기를 크게 확장한 것입니다. 언어 모델은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'요.Large Language Model은 언어 모델의 크기를 크게 확장한 것입니다. 언어 모델은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출하여 인간과 상호 작용할 수 있는 컴퓨터 시스템을 만드는 것을 목표로 합니다.Large Language Model은 이러한 NLP의 한 유형입니다. 이 모델은 언어 모델의 크기를 크게 확장하여 더 많은 데이터를 학습하고 더 큰 데이터 세트에 대해 더 잘 작동하도록 설계되었습니다.Large Language Model은 텍스트를 이해하고 해석하는 데 사용되는 인공 지능입니다. 예를 들어, 자연어 처리(NLP)는 텍스트에서 의미를 추출'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Large Language Model에 대해 설명해줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "], [Document(page_content='Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}), Document(page_content='Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}), Document(page_content='Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}), Document(page_content='Elasticsearch Relevance Engine™(ES"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'docs': [Document(page_content='ESRE는 검색 분야를 선도하는 Elastic의 리더십과 2년에 걸친 머신 러닝 연구 및 개발을 기반으로 구축되었습니다\\\\. Elasticsearch Relevance Engine은 AI의 장점과 Elastic의 텍스트 검색을 결합합니다\\\\. ESRE는 개발자들에게 정교한 검색 알고리즘의 전체 제품군과 대규모 언어 모델(LLM)과 통합할 수 있는 기능을 제공합니다', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}),\n",
       "  Document(page_content='즉, 사용자의 의도를 이해하여 이전에는 볼 수 없었던 구체적인 응답을 제공할 수 있습니다.특히, 모든 검색의 발전은 새로운 기술과 변화하는 사용자 행동으로 인해 제기된 새로운 문제를 해결하는 동시에 더 나은 정확도를 제공합니다\\\\. 시맨틱 검색을 제공하기 위해 키워드 검색을 확장하든 비디오 및 이미지에 대한 새로운 검색 양식을 활성화하든, 새로운 기술에는 검색 사용자에게 더 나은 경험을 제공하기 위한 고유한 도구가 필요합니다\\\\. 마찬가지로 오늘날의 인공 지능 세계에서는 고객이 테스트한 검증된 기능을 갖춘 기술 스택에 구축된 확장성이 뛰어난 새로운 개발자 툴킷이 필요합니다.생성형 AI의 추진력과 더불어 ChatGPT와 같은 기술 채택이 증가하고, 대규모 언어 모델 기능에 대한 인식이 높아지면서 개발자는 애플리케이션을 개선할 수 있는 기술을 실험하고 싶어 합니다\\\\. Elasticsearch Relevance Engine은 생성형 AI 세계에서 새로운 기능의 시대를 열었으며 모든 개발자 팀이 즉시 사용할 수 있는 강력한 도구를 제공합니다.Elasticsearch Relevance Engine은 Elastic Cloud에서 지금 제공되고 있습니다\\\\. 최신 릴리즈의 모든 새로운 기능들은 유일한 호스트형 Elasticsearch 제품인 Elastic Cloud에서만 이용하실 수 있습니다', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}),\n",
       "  Document(page_content='All rights reserved.Elastic, Elasticsearch 및 기타 관련 상표는 미국 및 기타 국가에서 Elasticsearch B.V.의 상표, 로고 또는 등록 상표입니다.Apache, Apache Lucene, Apache Hadoop, Hadoop, HDFS, 노란색 코끼리 로고는 미국 및/또는 기타 국가에서 Apache Software Foundation의 상표입니다.', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}),\n",
       "  Document(page_content='지금까지는 핵심 구성 요소인 RRF를 놓치고 있었습니다.\\xa0 이제 애플리케이션 검색 요구 사항에 RRF를 포함하고 있으므로 벡터 및 텍스트 검색 기능을 결합할 수 있습니다.\\xa0머신 러닝은 시맨틱 컨텍스트로 검색 결과의 정확도를 높이는 데 앞장서고 있지만, 비용, 복잡성 및 리소스 요구 사항이 너무 많아 개발자가 이를 효과적으로 구현하는 데 어려움을 겪고 있습니다\\\\. 개발자가 매우 정확한 AI 지원 검색을 구축하기 위해서는 전문 머신 러닝 또는 데이터 과학 팀의 지원이 필요한 경우가 일반적입니다\\\\. 이러한 팀은 적절한 모델을 선택하고, 도메인별 데이터 세트로 훈련하고, 데이터 및 해당 관계의 변경으로 인해 모델이 발전함에 따라 모델을 유지 관리하는 데 상당한 시간을 소비합니다.Go1이 확장 가능한 시맨틱 검색을 위해 어떻게 Elastic의 벡터 데이터베이스를 사용하는지 알아보세요.전문 팀의 지원을 받지 못하는 개발자도 시맨틱 검색을 구현할 수 있으며, 다른 대안에 필요한 노력과 전문 지식 없이 처음부터 AI 지원 검색 정확도의 이점을 누릴 수 있습니다\\\\. 오늘부터 모든 고객은 더 뛰어난 정확도와 현대적이고 스마트한 검색을 실현하는 데 도움이 되는 빌딩 블록을 갖추게 됩니다.사용해 보기이러한 기능과 그 외 다양한 기능에 대해 알아보세요.기존 Elastic Cloud 고객은 Elastic Cloud 콘솔에서 이 중 많은 기능을 바로 이용하실 수 있습니다\\\\. 클라우드에서 Elastic을 활용하고 있지 않으신가요? LLM 및 생성형 AI와 함께 Elasticsearch를 사용하는 방법을 확인해 보세요.이 게시물에 설명된 기능의 릴리즈 및 시기는 Elastic의 단독 재량에 따릅니다', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'})],\n",
       " 'text': \"], [Document(page_content='Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}), Document(page_content='Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}), Document(page_content='Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', metadata={'source': 'https://www.elastic.co/kr/blog/may-2023-launch-announcement', 'title': 'AI 검색 애플리케이션 구축을 위한 Elasticsearch Relevance Engine™ | Elastic Blog', 'description': 'Elasticsearch Relevance Engine™(ESRE)은 더 많은 검색 애플리케이션 개발자에게 더 높은 정확도를 제공하는 시맨틱 검색을 위한 벡터 데이터베이스와 머신 러닝 모델을 사용하여 프라이빗 데이터 세트를 위한 생성형 AI(인공 지능) 솔루션을 지원합니다.', 'language': 'ko-kr'}), Document(page_content='Elasticsearch Relevance Engine™(ES\"}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "# Chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run\n",
    "question = \"Elasticsearch에 LLM을 적용하는 방법을 알려줘\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "result = llm_chain(docs)\n",
    "\n",
    "# Output\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\es-lab-kr\\s-core\\quickstart.ipynb 셀 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/es-lab-kr/s-core/quickstart.ipynb#X36sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m qa_chain \u001b[39m=\u001b[39m RetrievalQA\u001b[39m.\u001b[39mfrom_chain_type(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/es-lab-kr/s-core/quickstart.ipynb#X36sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     llm,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/es-lab-kr/s-core/quickstart.ipynb#X36sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     retriever\u001b[39m=\u001b[39mdb\u001b[39m.\u001b[39mas_retriever(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/es-lab-kr/s-core/quickstart.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     chain_type_kwargs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m: prompt}\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/es-lab-kr/s-core/quickstart.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/es-lab-kr/s-core/quickstart.ipynb#X36sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mElasticsearch에 LLM을 적용하는 방법을 알려줄 수 있니?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/es-lab-kr/s-core/quickstart.ipynb#X36sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m result \u001b[39m=\u001b[39m qa_chain({\u001b[39m\"\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m\"\u001b[39;49m: question})\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\chains\\base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    314\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\chains\\base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    299\u001b[0m     inputs,\n\u001b[0;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[0;32m    301\u001b[0m )\n\u001b[0;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 304\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    307\u001b[0m     )\n\u001b[0;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:139\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_docs(question)  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_documents_chain\u001b[39m.\u001b[39;49mrun(\n\u001b[0;32m    140\u001b[0m     input_documents\u001b[39m=\u001b[39;49mdocs, question\u001b[39m=\u001b[39;49mquestion, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child()\n\u001b[0;32m    141\u001b[0m )\n\u001b[0;32m    143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_source_documents:\n\u001b[0;32m    144\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: answer, \u001b[39m\"\u001b[39m\u001b[39msource_documents\u001b[39m\u001b[39m\"\u001b[39m: docs}\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\chains\\base.py:510\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[0;32m    506\u001b[0m         _output_key\n\u001b[0;32m    507\u001b[0m     ]\n\u001b[0;32m    509\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m--> 510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[0;32m    511\u001b[0m         _output_key\n\u001b[0;32m    512\u001b[0m     ]\n\u001b[0;32m    514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    515\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    516\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    517\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but none were provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    518\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\chains\\base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    314\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\chains\\base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    299\u001b[0m     inputs,\n\u001b[0;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[0;32m    301\u001b[0m )\n\u001b[0;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 304\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    307\u001b[0m     )\n\u001b[0;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:122\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    121\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[1;32m--> 122\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine_docs(\n\u001b[0;32m    123\u001b[0m     docs, callbacks\u001b[39m=\u001b[39m_run_manager\u001b[39m.\u001b[39mget_child(), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mother_keys\n\u001b[0;32m    124\u001b[0m )\n\u001b[0;32m    125\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[0;32m    126\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py:171\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[1;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_inputs(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[39m# Call predict on the LLM.\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mpredict(callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs), {}\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\chains\\llm.py:257\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    243\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \n\u001b[0;32m    245\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 257\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\chains\\base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    314\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\chains\\base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    299\u001b[0m     inputs,\n\u001b[0;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[0;32m    301\u001b[0m )\n\u001b[0;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 304\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    307\u001b[0m     )\n\u001b[0;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\chains\\llm.py:93\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[0;32m     89\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     90\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m     91\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     92\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m---> 93\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m     94\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\chains\\llm.py:103\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    104\u001b[0m     prompts,\n\u001b[0;32m    105\u001b[0m     stop,\n\u001b[0;32m    106\u001b[0m     callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    107\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_kwargs,\n\u001b[0;32m    108\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\llms\\base.py:497\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[0;32m    490\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    491\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    495\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    496\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[1;32m--> 497\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(prompt_strings, stop\u001b[39m=\u001b[39mstop, callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\llms\\base.py:646\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    632\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    633\u001b[0m         )\n\u001b[0;32m    634\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[0;32m    635\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[0;32m    636\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    644\u001b[0m         )\n\u001b[0;32m    645\u001b[0m     ]\n\u001b[1;32m--> 646\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_helper(\n\u001b[0;32m    647\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39m(new_arg_supported), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    648\u001b[0m     )\n\u001b[0;32m    649\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[0;32m    650\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\llms\\base.py:534\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[0;32m    533\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 534\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    535\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m    536\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\llms\\base.py:521\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[0;32m    512\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    513\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    518\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    519\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 521\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(\n\u001b[0;32m    522\u001b[0m                 prompts,\n\u001b[0;32m    523\u001b[0m                 stop\u001b[39m=\u001b[39mstop,\n\u001b[0;32m    524\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    525\u001b[0m                 run_manager\u001b[39m=\u001b[39mrun_managers[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m run_managers \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    526\u001b[0m                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    527\u001b[0m             )\n\u001b[0;32m    528\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    529\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[0;32m    530\u001b[0m         )\n\u001b[0;32m    531\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    532\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\llms\\base.py:1043\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1041\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[0;32m   1042\u001b[0m     text \u001b[39m=\u001b[39m (\n\u001b[1;32m-> 1043\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1044\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1045\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1046\u001b[0m     )\n\u001b[0;32m   1047\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[0;32m   1048\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\llms\\llamacpp.py:291\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstreaming:\n\u001b[0;32m    287\u001b[0m     \u001b[39m# If streaming is enabled, we use the stream\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[39m# method that yields as they are generated\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     \u001b[39m# and return the combined strings from the first choices's text:\u001b[39;00m\n\u001b[0;32m    290\u001b[0m     combined_text_output \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 291\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream(\n\u001b[0;32m    292\u001b[0m         prompt\u001b[39m=\u001b[39mprompt,\n\u001b[0;32m    293\u001b[0m         stop\u001b[39m=\u001b[39mstop,\n\u001b[0;32m    294\u001b[0m         run_manager\u001b[39m=\u001b[39mrun_manager,\n\u001b[0;32m    295\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    296\u001b[0m     ):\n\u001b[0;32m    297\u001b[0m         combined_text_output \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m chunk\u001b[39m.\u001b[39mtext\n\u001b[0;32m    298\u001b[0m     \u001b[39mreturn\u001b[39;00m combined_text_output\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\llms\\llamacpp.py:344\u001b[0m, in \u001b[0;36mLlamaCpp._stream\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    342\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_parameters(stop), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m    343\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient(prompt\u001b[39m=\u001b[39mprompt, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m--> 344\u001b[0m \u001b[39mfor\u001b[39;00m part \u001b[39min\u001b[39;00m result:\n\u001b[0;32m    345\u001b[0m     logprobs \u001b[39m=\u001b[39m part[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    346\u001b[0m     chunk \u001b[39m=\u001b[39m GenerationChunk(\n\u001b[0;32m    347\u001b[0m         text\u001b[39m=\u001b[39mpart[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m    348\u001b[0m         generation_info\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m\"\u001b[39m: logprobs},\n\u001b[0;32m    349\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\llama_cpp\\llama.py:991\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001b[0m\n\u001b[0;32m    989\u001b[0m finish_reason \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    990\u001b[0m multibyte_fix \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 991\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[0;32m    992\u001b[0m     prompt_tokens,\n\u001b[0;32m    993\u001b[0m     top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[0;32m    994\u001b[0m     top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m    995\u001b[0m     temp\u001b[39m=\u001b[39mtemperature,\n\u001b[0;32m    996\u001b[0m     tfs_z\u001b[39m=\u001b[39mtfs_z,\n\u001b[0;32m    997\u001b[0m     mirostat_mode\u001b[39m=\u001b[39mmirostat_mode,\n\u001b[0;32m    998\u001b[0m     mirostat_tau\u001b[39m=\u001b[39mmirostat_tau,\n\u001b[0;32m    999\u001b[0m     mirostat_eta\u001b[39m=\u001b[39mmirostat_eta,\n\u001b[0;32m   1000\u001b[0m     frequency_penalty\u001b[39m=\u001b[39mfrequency_penalty,\n\u001b[0;32m   1001\u001b[0m     presence_penalty\u001b[39m=\u001b[39mpresence_penalty,\n\u001b[0;32m   1002\u001b[0m     repeat_penalty\u001b[39m=\u001b[39mrepeat_penalty,\n\u001b[0;32m   1003\u001b[0m     stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1004\u001b[0m     logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1005\u001b[0m     grammar\u001b[39m=\u001b[39mgrammar,\n\u001b[0;32m   1006\u001b[0m ):\n\u001b[0;32m   1007\u001b[0m     \u001b[39mif\u001b[39;00m token \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_token_eos:\n\u001b[0;32m   1008\u001b[0m         text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\llama_cpp\\llama.py:806\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, tokens, top_k, top_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[0;32m    803\u001b[0m     grammar\u001b[39m.\u001b[39mreset()\n\u001b[0;32m    805\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 806\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(tokens)\n\u001b[0;32m    807\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[0;32m    808\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[0;32m    809\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    819\u001b[0m         grammar\u001b[39m=\u001b[39mgrammar,\n\u001b[0;32m    820\u001b[0m     )\n\u001b[0;32m    821\u001b[0m     \u001b[39mif\u001b[39;00m stopping_criteria \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m stopping_criteria(\n\u001b[0;32m    822\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_ids, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scores[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[0;32m    823\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\llama_cpp\\llama.py:534\u001b[0m, in \u001b[0;36mLlama.eval\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    532\u001b[0m n_past \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_ctx \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(batch), \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_ids))\n\u001b[0;32m    533\u001b[0m n_tokens \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch)\n\u001b[1;32m--> 534\u001b[0m return_code \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39;49mllama_eval(\n\u001b[0;32m    535\u001b[0m     ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx,\n\u001b[0;32m    536\u001b[0m     tokens\u001b[39m=\u001b[39;49m(llama_cpp\u001b[39m.\u001b[39;49mllama_token \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(batch))(\u001b[39m*\u001b[39;49mbatch),\n\u001b[0;32m    537\u001b[0m     n_tokens\u001b[39m=\u001b[39;49mn_tokens,\n\u001b[0;32m    538\u001b[0m     n_past\u001b[39m=\u001b[39;49mn_past,\n\u001b[0;32m    539\u001b[0m )\n\u001b[0;32m    540\u001b[0m \u001b[39mif\u001b[39;00m return_code \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    541\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mllama_eval returned \u001b[39m\u001b[39m{\u001b[39;00mreturn_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\chatbot\\lib\\site-packages\\llama_cpp\\llama_cpp.py:999\u001b[0m, in \u001b[0;36mllama_eval\u001b[1;34m(ctx, tokens, n_tokens, n_past)\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mllama_eval\u001b[39m(\n\u001b[0;32m    994\u001b[0m     ctx: llama_context_p,\n\u001b[0;32m    995\u001b[0m     tokens,  \u001b[39m# type: Array[llama_token]\u001b[39;00m\n\u001b[0;32m    996\u001b[0m     n_tokens: Union[c_int, \u001b[39mint\u001b[39m],\n\u001b[0;32m    997\u001b[0m     n_past: Union[c_int, \u001b[39mint\u001b[39m],\n\u001b[0;32m    998\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m--> 999\u001b[0m     \u001b[39mreturn\u001b[39;00m _lib\u001b[39m.\u001b[39;49mllama_eval(ctx, tokens, n_tokens, n_past)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "template = \"\"\"[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>> \n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer: [/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
    "\n",
    "# RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "question = \"Elasticsearch에 LLM을 적용하는 방법을 알려줄 수 있니?\"\n",
    "result = qa_chain({\"query\": question})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
