{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG using local models\n",
    "\n",
    "\n",
    "\n",
    "https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4 tiktoken langchain elasticsearch eland[pytorch]==8.10.1 --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "ES_URL = input('Elasticsearch URL(ex:https://127.0.0.1:9200): ')\n",
    "ES_USER = \"elastic\" \n",
    "ES_USER_PASSWORD = getpass('elastic user PW: ')\n",
    "CERT_PATH = input('Elasticsearch pem path: ')\n",
    "\n",
    "client = Elasticsearch(\n",
    "    ES_URL,\n",
    "    basic_auth=(ES_USER, ES_USER_PASSWORD),\n",
    "    ca_certs=CERT_PATH,\n",
    "    request_timeout=60\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "try :\n",
    "    os.mkdir(cwd + \"/models\") \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(cwd + \"/models\")\n",
    "\n",
    "try :\n",
    "    os.system(\"git clone https://huggingface.co/intfloat/multilingual-e5-base\")\n",
    "except:\n",
    "    print('이미 모델이 존재합니다.')\n",
    "\n",
    "os.chdir(cwd)\n",
    "\n",
    "es_model_id = \"intfloat_multilingual_efive_base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from eland.ml import MLModel\n",
    "from eland.ml.pytorch import PyTorchModel\n",
    "from eland.common import es_version\n",
    "from eland.ml.pytorch.transformers import TransformerModel\n",
    "\n",
    "es_model = MLModel(client, es_model_id)\n",
    "\n",
    "if (es_model.exists_model() == False) :\n",
    "    # 현재 경로 얻기\n",
    "    cwd = os.getcwd()\n",
    "    local_model_path = cwd + '/models/multilingual-e5-base'\n",
    "\n",
    "    print(local_model_path)\n",
    "\n",
    "    # 모델 이름 및 작업 유형 설정\n",
    "    #tm = TransformerModel(local_model_path, \"text_embedding\")\n",
    "    es_cluster_version = es_version(client)\n",
    "    tm = TransformerModel(\n",
    "        model_id=local_model_path, \n",
    "        task_type=\"text_embedding\", \n",
    "        es_version=es_cluster_version\n",
    "    )\n",
    "    tmp_path = \"tmp_models/\" + es_model_id \n",
    "    Path(tmp_path).mkdir(parents=True, exist_ok=True)\n",
    "    model_path, config, vocab_path = tm.save(tmp_path)\n",
    "\n",
    "    print(tmp_path)\n",
    "\n",
    "    ptm = PyTorchModel(client, es_model_id)\n",
    "    ptm.import_model(\n",
    "        model_path=model_path,\n",
    "        config_path=None,\n",
    "        vocab_path=vocab_path,\n",
    "        config=config\n",
    "    ) \n",
    "    ptm.start()\n",
    "\n",
    "    shutil.rmtree(Path(\"tmp_models\"), ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader([\n",
    "    \"https://www.elastic.co/search-labs/blog/articles/may-2023-launch-announcement\",\n",
    "    \"https://www.elastic.co/kr/blog/may-2023-launch-announcement\"\n",
    "])\n",
    "loader.requests_kwargs = {'verify':False}\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=0, \n",
    "    separator=\". \",\n",
    "    length_function = len\n",
    ")\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import ElasticsearchStore\n",
    "from langchain.embeddings.elasticsearch import ElasticsearchEmbeddings\n",
    "\n",
    "embeddings = ElasticsearchEmbeddings.from_es_connection(\n",
    "    es_connection=client,\n",
    "    model_id = es_model_id\n",
    ")\n",
    "\n",
    "vectorstore = ElasticsearchStore(\n",
    "    es_connection=client, \n",
    "    embedding=embeddings, \n",
    "    query_field=\"text_field\",\n",
    "    vector_query_field=\"vector_query_field.predicted_value\",\n",
    "    index_name= \"workplace_index\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID=\"vectorize_workplace\"\n",
    "\n",
    "vectorstore.client.ingest.put_pipeline(id=PIPELINE_ID, processors=[{\n",
    "  \"inference\": {\n",
    "    \"model_id\": es_model_id,\n",
    "    \"field_map\": {\n",
    "      \"query_field\": \"text_field\"\n",
    "    },\n",
    "      \"target_field\": \"vector_query_field\",\n",
    "  }\n",
    "}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"workplace_index\"\n",
    "\n",
    "# define index mapping\n",
    "INDEX_MAPPING = {\n",
    "    \"properties\": {\n",
    "        \"text_field\": {\n",
    "            \"type\": \"text\"\n",
    "        },\n",
    "        \"vector_query_field\": {\n",
    "            \"properties\": {\n",
    "                \"is_truncated\": {\n",
    "                    \"type\": \"boolean\"\n",
    "                },\n",
    "                \"predicted_value\": {\n",
    "                    \"type\": \"dense_vector\",\n",
    "                    \"dims\": 768,\n",
    "                    \"index\": True,\n",
    "                    \"similarity\": \"cosine\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "INDEX_SETTINGS = {\"index\": { \"default_pipeline\": PIPELINE_ID}}\n",
    "\n",
    "if vectorstore.client.indices.exists(index=INDEX_NAME):\n",
    "    print(\"Deleting existing %s\" % INDEX_NAME)\n",
    "    vectorstore.client.indices.delete(index=INDEX_NAME, ignore=[400, 404])\n",
    "\n",
    "print(\"Creating index %s\" % INDEX_NAME)\n",
    "vectorstore.client.indices.create(index=INDEX_NAME, mappings=INDEX_MAPPING, settings=INDEX_SETTINGS,\n",
    "                  ignore=[400, 404])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = ElasticsearchStore.from_documents(\n",
    "    docs, \n",
    "    es_connection=client,\n",
    "    query_field=\"text_field\",\n",
    "    vector_query_field=\"vector_query_field.predicted_value\",\n",
    "    index_name= \"workplace_index\",\n",
    "    distance_strategy=\"COSINE\",\n",
    "    strategy=ElasticsearchStore.ApproxRetrievalStrategy(\n",
    "        hybrid=True,\n",
    "        query_model_id=es_model_id\n",
    "    ),\n",
    "    bulk_kwargs={\n",
    "        \"chunk_size\": 10,\n",
    "        \"max_chunk_bytes\": 200000000\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showResults(output):\n",
    "  print(\"Total results: \", len(output))\n",
    "  for index in range(len(output)):\n",
    "    print(output[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showResults(db.similarity_search(\"セマンティック検索 RRF\", k=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"LLM의 역할을 알려줘?\"\n",
    "results = db.similarity_search(\n",
    "    query\n",
    ")\n",
    "\n",
    "showResults(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path = cwd + \"/models/Llama-2-ko-7B-chat-gguf-q8_0.bin\",\n",
    "    # n_gpu_layers=n_gpu_layers,\n",
    "    # n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "\n",
    "    # https://www.reddit.com/r/LocalLLaMA/comments/1343bgz/what_model_parameters_is_everyone_using/\n",
    "    temperature=0.7,\n",
    "    top_k=2,\n",
    "    top_p=0.1,\n",
    "\n",
    "    max_tokens=512,\n",
    "    verbose=True,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"Large Language Model에 대해 설명해줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "# Chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run\n",
    "question = \"Elasticsearch에 LLM을 적용하는 방법을 알려줘\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "result = llm_chain(docs)\n",
    "\n",
    "# Output\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "Act as a cryptocurrency expert. Use the following information to answer the question at the end.\n",
    "<</SYS>>\n",
    " \n",
    "{context}\n",
    " \n",
    "{question} [/INST]\n",
    "\"\"\"\n",
    " \n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    " \n",
    " \n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    " \n",
    "result = qa_chain(\n",
    "    question\n",
    ")\n",
    "print(result[\"result\"].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
